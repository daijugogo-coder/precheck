import streamlit as st
import pandas as pd
import io
import unicodedata
from typing import Dict, List
import csv
import hashlib
import re
from dataclasses import dataclass
from datetime import datetime, date
from io import StringIO
from typing import Optional, Tuple

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="åœ¨åº«ä¸è¶³ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“¦",
    layout="wide"
)

st.title("ğŸ“¦ åœ¨åº«ä¸è¶³ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("---")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None


def safe_rerun() -> None:
    """Rerun the Streamlit script in a way compatible with multiple Streamlit versions."""
    # Preferred API
    if hasattr(st, "experimental_rerun"):
        try:
            st.experimental_rerun()
            return
        except Exception:
            pass

    # Fallback: raise the internal RerunException
    try:
        from streamlit.runtime.scriptrunner.script_runner import RerunException
        raise RerunException()
    except Exception:
        # As last resort, use stop to prevent further UI actions
        try:
            st.stop()
        except Exception:
            return

def load_csv_with_encoding(file, use_lf=True, encoding='cp932') -> pd.DataFrame:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã‚’æŒ‡å®šï¼‰
    """
    try:
        content = file.read()
        decoded_content = content.decode(encoding)
        if use_lf:
            df = pd.read_csv(io.StringIO(decoded_content), lineterminator='\n')
        else:
            df = pd.read_csv(io.StringIO(decoded_content))
        return df
    except Exception as e:
        st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def load_master_files(master_857001, master_857002, master_857003) -> Dict[str, pd.DataFrame]:
    """
    3ã¤ã®ãƒã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆUTF-8ã€CRLFæ”¹è¡Œï¼‰
    """
    masters = {}
    
    if master_857001:
        df = load_csv_with_encoding(master_857001, use_lf=False, encoding='utf-8')
        if df is not None:
            # ãƒã‚¹ã‚¿857001ã®åŠ å·¥
            df = df[['å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤01', 'ã‚³ãƒ¼ãƒ‰å€¤1', 'ã‚³ãƒ¼ãƒ‰å€¤2', 'ã‚³ãƒ¼ãƒ‰å€¤4']].copy()
            df = df.rename(columns={
                'å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤01': 'å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰',
                'ã‚³ãƒ¼ãƒ‰å€¤1': 'åº—èˆ—å€‰åº«åŒºåˆ†',
                'ã‚³ãƒ¼ãƒ‰å€¤2': 'äº‹æ¥­CDï½‚ï½‹',
                'ã‚³ãƒ¼ãƒ‰å€¤4': 'ä¿ç®¡å ´æ‰€CD'
            })
            # äº‹æ¥­CDã®è¨ˆç®—: TGã§å§‹ã¾ã‚‹ãªã‚‰13000ã€ãã‚Œä»¥å¤–ã¯15000
            df['äº‹æ¥­CD'] = df['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'].apply(lambda x: '13000' if str(x).startswith('TG') else '15000')
            df = df.drop_duplicates(subset=['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'])
            masters['857001'] = df
            st.success(f"âœ… ãƒã‚¹ã‚¿857001èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")
    
    if master_857002:
        df = load_csv_with_encoding(master_857002, use_lf=False, encoding='utf-8')
        if df is not None:
            # ãƒã‚¹ã‚¿857002ã®åŠ å·¥
            df = df[['å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤01', 'å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤02', 'ã‚³ãƒ¼ãƒ‰å€¤1']].copy()
            df = df.rename(columns={
                'å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤01': 'å•†å“ã‚³ãƒ¼ãƒ‰',
                'å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤02': 'äº‹æ¥­CD',
                'ã‚³ãƒ¼ãƒ‰å€¤1': 'TMSå•†å“CD'
            })
            df = df.drop_duplicates(subset=['å•†å“ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­CD'])
            masters['857002'] = df
            st.success(f"âœ… ãƒã‚¹ã‚¿857002èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")
    
    if master_857003:
        df = load_csv_with_encoding(master_857003, use_lf=False, encoding='utf-8')
        if df is not None:
            masters['857003'] = df
            st.success(f"âœ… ãƒã‚¹ã‚¿857003èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")
    
    return masters


### --- ä»¥ä¸‹ main.py ã‹ã‚‰å–ã‚Šè¾¼ã‚“ã ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° (å£²ä¸Šå‰å‡¦ç†ç”¨) ---
def drop_ag_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    Excelã®AGåˆ— = 33åˆ—ç›®ï¼ˆ1-basedï¼‰= index 32ï¼ˆ0-basedï¼‰
    å­˜åœ¨ã™ã‚Œã°å‰Šé™¤ã™ã‚‹ã€‚CSVã§ã‚‚åˆ—æ•°ãŒå¤šã‘ã‚Œã°åŒæ§˜ã«å‹•ä½œã™ã‚‹ã€‚
    """
    ag_index = 32
    if df.shape[1] <= ag_index:
        return df
    return df.drop(df.columns[ag_index], axis=1)


def normalize_text(s: str) -> str:
    """åŠè§’ãƒ»å…¨è§’ã®æºã‚Œã‚’å¸åã™ã‚‹ãŸã‚ã«æ­£è¦åŒ–ã™ã‚‹ï¼ˆNFKCï¼‰"""
    return unicodedata.normalize("NFKC", s)


def split_docomo_shop_rows(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Dåˆ—ï¼ˆ4åˆ—ç›®ï¼‰ã«ã€Œãƒ‰ã‚³ãƒ¢ã‚·ãƒ§ãƒƒãƒ—ï¼ˆåŠè§’å…¨è§’å•ã‚ãšï¼‰ã€ã‚’å«ã‚€è¡Œã‚’æ®‹ã—ã€
    ãã‚Œä»¥å¤–ã‚’ omitï¼ˆé™¤å¤–ï¼‰ã¨ã—ã¦åˆ†é›¢ã™ã‚‹ã€‚
    å®‰å…¨ã®ãŸã‚ã€å¯¾è±¡åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™ã€‚
    """
    col_index = 3  # Dåˆ—ï¼ˆ0-basedï¼‰
    if df.shape[1] <= col_index:
        # å¯¾è±¡åˆ—ãŒãªã„ã®ã§åˆ†é›¢ã¯è¡Œã‚ãªã„
        return df.copy(), pd.DataFrame()

    keyword = normalize_text("ãƒ‰ã‚³ãƒ¢ã‚·ãƒ§ãƒƒãƒ—")

    series = (
        df.iloc[:, col_index]
        .astype("string")
        .fillna("")
        .map(normalize_text)
    )

    mask = series.str.contains(keyword, na=False)

    kept_df = df[mask].copy()
    omitted_df = df[~mask].copy()

    return kept_df, omitted_df


# --- main.py ã®ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½å–ã‚Šè¾¼ã¿ ---
TARGET_COL_25 = 24
TARGET_COL_38 = 37
DATE_COL_9 = 8
DATE_COL_17 = 16
DATE_TIME_RE = re.compile(r"^\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}$")


@dataclass
class ErrorDetail:
    row: int
    store_name: str
    slip_number: str
    col_38: str


@dataclass
class DateIssue:
    record_no: int
    start_physical_line: int
    severity: str
    issue_type: str
    col9: str
    col17: str
    note: str


@dataclass
class DateSummary:
    total_checked_cells: int
    count_col9_ok: int
    count_error: int
    issues: List[DateIssue]


def csv_reader_from_text(csv_text: str):
    return csv.reader(StringIO(csv_text, newline=""))


def parse_dt_str(s: str) -> Optional[datetime]:
    t = s.strip()
    if not DATE_TIME_RE.match(t):
        return None
    try:
        return datetime.strptime(t, "%Y/%m/%d %H:%M:%S")
    except Exception:
        return None


def build_error_csv_bytes(details: List[ErrorDetail]) -> bytes:
    buf = StringIO()
    w = csv.writer(buf, lineterminator="\n")
    w.writerow(["è¡Œç•ªå·(ç‰©ç†è¡Œ)", "åº—èˆ—å", "ä¼ç¥¨ç•ªå·", "é‡‘é¡(38åˆ—ç›®)"])
    for d in details:
        w.writerow([d.row, d.store_name, d.slip_number, d.col_38])
    return buf.getvalue().encode("utf-8")


def build_date_issue_csv_bytes(issues: List[DateIssue]) -> bytes:
    buf = StringIO()
    w = csv.writer(buf, lineterminator="\n")
    w.writerow(["ãƒ¬ã‚³ãƒ¼ãƒ‰ç•ªå·", "é–‹å§‹ç‰©ç†è¡Œ(å‚è€ƒ)", "é‡è¦åº¦", "ç¨®åˆ¥", "9åˆ—ç›®", "17åˆ—ç›®", "è£œè¶³"])
    for it in issues:
        w.writerow([it.record_no, it.start_physical_line, it.severity, it.issue_type, it.col9, it.col17, it.note])
    return buf.getvalue().encode("utf-8")


def check_and_analyze(csv_text: str) -> Tuple[bool, List[ErrorDetail], int, int, DateSummary]:
    error_details: List[ErrorDetail] = []
    total_data_records = 0
    total_physical_lines = 0

    total_checked_cells = 0
    count_col9_ok = 0
    count_error = 0
    issues: List[DateIssue] = []

    reader = csv_reader_from_text(csv_text)
    prev_end_line = 0

    for record_no, row in enumerate(reader, start=1):
        start_physical_line = prev_end_line + 1
        end_physical_line = reader.line_num
        prev_end_line = end_physical_line
        total_physical_lines = end_physical_line

        # skip header
        if record_no == 1:
            continue

        total_data_records += 1

        # NGãƒã‚§ãƒƒã‚¯ 25/38
        if len(row) >= (TARGET_COL_38 + 1):
            col_3 = row[2].strip() if len(row) > 2 else ""
            col_11 = row[10].strip() if len(row) > 10 else ""
            col_25 = row[TARGET_COL_25].strip() if len(row) > TARGET_COL_25 else ""
            col_38 = row[TARGET_COL_38].strip() if len(row) > TARGET_COL_38 else ""

            if col_25 == "Z00014" and col_38 not in {"3000", "5000"}:
                error_details.append(ErrorDetail(row=start_physical_line, store_name=col_3, slip_number=col_11, col_38=col_38))

        # date checks
        col9 = row[DATE_COL_9].strip() if len(row) > DATE_COL_9 else ""
        dt9 = parse_dt_str(col9)
        if dt9 is None:
            count_error += 1
            issues.append(DateIssue(record_no=total_data_records, start_physical_line=start_physical_line, severity="ERROR", issue_type="COL9_MISSING_OR_INVALID", col9=col9, col17="", note="9åˆ—ç›®ã« yyyy/mm/dd hh:mm:ss ãŒå¿…è¦ã§ã™ã€‚"))
        else:
            count_col9_ok += 1
            total_checked_cells += 1

    date_summary = DateSummary(total_checked_cells=total_checked_cells, count_col9_ok=count_col9_ok, count_error=count_error, issues=issues)

    return (len(error_details) > 0), error_details, total_data_records, total_physical_lines, date_summary

# --- end of main.py checks ---

### --- å–ã‚Šè¾¼ã¿ã“ã“ã¾ã§ ---

def process_shiire_data(df: pd.DataFrame, masters: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    ä»•å…¥ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨å‡¦ç†ï¼ˆPower Queryæº–æ‹ ï¼‰
    """
    if df is None or df.empty:
        return pd.DataFrame()
    
    # å‹å¤‰æ›
    df['å—æ‰•å‰åœ¨åº«æ•°'] = pd.to_numeric(df['å—æ‰•å‰åœ¨åº«æ•°'], errors='coerce').fillna(0).astype(int)
    df['æ•°é‡'] = pd.to_numeric(df['æ•°é‡'], errors='coerce').fillna(0).astype(int)
    df['å—æ‰•å¾Œåœ¨åº«æ•°'] = pd.to_numeric(df['å—æ‰•å¾Œåœ¨åº«æ•°'], errors='coerce').fillna(0).astype(int)
    
    # ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆInner Joinï¼‰
    if '857001' in masters:
        df = df.merge(masters['857001'], on='å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='inner')
    
    # åº—èˆ—å€‰åº«åŒºåˆ†ã§ãƒ•ã‚£ãƒ«ã‚¿
    df = df[df['åº—èˆ—å€‰åº«åŒºåˆ†'] == '1']
    
    # ãƒã‚¹ã‚¿857002ã¨ãƒãƒ¼ã‚¸ï¼ˆLeft Outer Joinï¼‰
    if '857002' in masters:
        # å‹ã‚’çµ±ä¸€ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰
        df['äº‹æ¥­CD'] = df['äº‹æ¥­CD'].astype(str)
        master_857002 = masters['857002'].copy()
        master_857002['äº‹æ¥­CD'] = master_857002['äº‹æ¥­CD'].astype(str)
        
        df = df.merge(master_857002, on=['äº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'], how='left', suffixes=('', '_master'))
        # TMSå•†å“CDãŒnullãªã‚‰å•†å“ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
        df['TMSå•†å“CD'] = df['TMSå•†å“CD'].fillna(df['å•†å“ã‚³ãƒ¼ãƒ‰'])
    else:
        df['TMSå•†å“CD'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    
    # æ•°é‡ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    df['æ•°é‡bk'] = df['æ•°é‡']
    
    # å—æ‰•ç¨®åˆ¥ã§ãƒ•ã‚£ãƒ«ã‚¿
    valid_types = ['å€‰åº«ã¸è¿”å“', 'å…¥è·', 'å…¥è·(ã‚·ã‚¹ãƒ†ãƒ è‡ªå‹•)', 'è¿”å“ã‚­ãƒ£ãƒ³ã‚»ãƒ«', 'è¿”å“ä¸å‚™']
    df = df[df['å—æ‰•ç¨®åˆ¥'].isin(valid_types)]
    
    # é™¤å¤–å•†å“ã‚³ãƒ¼ãƒ‰
    exclude_codes = ['ZUA292', 'ZUA34Q', 'ZUA34R', 'ZUA34S', 'ZUA34T', 'ZUA34U', 'ZUA34V', 'ZUA34W']
    for code in exclude_codes:
        df = df[~df['å•†å“ã‚³ãƒ¼ãƒ‰'].str.contains(code, na=False)]
    
    return df

def process_shiire_individual(df: pd.DataFrame) -> pd.DataFrame:
    """ä»•å…¥ãƒ‡ãƒ¼ã‚¿ï¼ˆå€‹ä½“æƒ…å ±ï¼‰"""
    # IMEIã€ICCIDã€ãã®ä»–ã‚·ãƒªã‚¢ãƒ«ã®ã„ãšã‚Œã‹ãŒã‚ã‚‹
    individual = df[
        (df['IMEI'].notna() & (df['IMEI'] != '')) |
        (df['ICCID'].notna() & (df['ICCID'] != '')) |
        (df['ãã®ä»–ã‚·ãƒªã‚¢ãƒ«'].notna() & (df['ãã®ä»–ã‚·ãƒªã‚¢ãƒ«'] != ''))
    ].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªä¸­ãŒï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰ã§ãªã„
    individual = individual[individual['ã‚«ãƒ†ã‚´ãƒªä¸­'] != 'ï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰']
    
    # æ•°é‡è¨ˆç®—: å€‰åº«ã¸è¿”å“ãªã‚‰-1ã€ãã‚Œä»¥å¤–ã¯1
    individual['æ•°é‡'] = individual['å—æ‰•ç¨®åˆ¥'].apply(lambda x: -1 if x == 'å€‰åº«ã¸è¿”å“' else 1)
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = individual.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    
    return result

def process_shiire_accessory(df: pd.DataFrame) -> pd.DataFrame:
    """ä»•å…¥ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¢ã‚¯ã‚»ã‚µãƒªï¼‰"""
    # IMEIã€ICCIDã€ãã®ä»–ã‚·ãƒªã‚¢ãƒ«å…¨ã¦ãŒç©º
    accessory = df[
        (df['IMEI'].isna() | (df['IMEI'] == '')) &
        (df['ICCID'].isna() | (df['ICCID'] == '')) &
        (df['ãã®ä»–ã‚·ãƒªã‚¢ãƒ«'].isna() | (df['ãã®ä»–ã‚·ãƒªã‚¢ãƒ«'] == ''))
    ].copy()
    
    # æ•°é‡bkã‚’ä½¿ç”¨
    accessory['æ•°é‡'] = accessory['æ•°é‡bk']
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = accessory.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    
    return result

def process_ido_data(df: pd.DataFrame, masters: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    ç§»å‹•ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨å‡¦ç†ï¼ˆPower Queryæº–æ‹ ï¼‰
    """
    if df is None or df.empty:
        return pd.DataFrame()
    
    # å‹å¤‰æ›
    df['å…¥åº«äºˆå®šæ•°'] = pd.to_numeric(df['å…¥åº«äºˆå®šæ•°'], errors='coerce').fillna(0).astype(int)
    df['æœªå…¥åº«æ•°'] = pd.to_numeric(df['æœªå…¥åº«æ•°'], errors='coerce').fillna(0).astype(int)
    
    # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤ï¼ˆç©ºã®åˆ—ï¼‰
    df = df.loc[:, ~df.columns.str.startswith('_')]
    df = df.loc[:, df.columns != '']
    
    # ç§»å‹•å…ƒå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰ã§ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆInner Joinï¼‰
    if '857001' in masters:
        moto_master = masters['857001'].copy()
        moto_master = moto_master.rename(columns={
            'å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰': 'ç§»å‹•å…ƒå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰',
            'åº—èˆ—å€‰åº«åŒºåˆ†': 'åº—èˆ—å€‰åº«åŒºåˆ†',
            'äº‹æ¥­CD': 'ç§»å‹•å…ƒäº‹æ¥­CD',
            'ä¿ç®¡å ´æ‰€CD': 'ç§»å‹•å…ƒä¿ç®¡å ´æ‰€CD'
        })
        df = df.merge(moto_master, on='ç§»å‹•å…ƒå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='inner')
    
    # ç§»å‹•å…ˆå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰ã§ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆInner Joinï¼‰
    if '857001' in masters:
        saki_master = masters['857001'][['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD']].copy()
        saki_master = saki_master.rename(columns={
            'å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰': 'ç§»å‹•å…ˆå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰',
            'äº‹æ¥­CD': 'ç§»å‹•å…ˆäº‹æ¥­CD',
            'ä¿ç®¡å ´æ‰€CD': 'ç§»å‹•å…ˆä¿ç®¡å ´æ‰€CD'
        })
        df = df.merge(saki_master, on='ç§»å‹•å…ˆå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='inner')
    
    # ãƒã‚¹ã‚¿857002ã¨ãƒãƒ¼ã‚¸
    if '857002' in masters:
        # å‹ã‚’çµ±ä¸€ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰
        df['ç§»å‹•å…ƒäº‹æ¥­CD'] = df['ç§»å‹•å…ƒäº‹æ¥­CD'].astype(str)
        master_857002 = masters['857002'].copy()
        master_857002['äº‹æ¥­CD'] = master_857002['äº‹æ¥­CD'].astype(str)
        
        df = df.merge(
            master_857002,
            left_on=['ç§»å‹•å…ƒäº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'],
            right_on=['äº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'],
            how='left',
            suffixes=('', '_master')
        )
        df['TMSå•†å“CD'] = df['TMSå•†å“CD'].fillna(df['å•†å“ã‚³ãƒ¼ãƒ‰'])
        if 'äº‹æ¥­CD_master' in df.columns:
            df = df.drop(columns=['äº‹æ¥­CD_master'])
    else:
        df['TMSå•†å“CD'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    
    return df

def process_ido_shukko(df: pd.DataFrame) -> pd.DataFrame:
    """ç§»å‹•ãƒ‡ãƒ¼ã‚¿ï¼ˆå‡ºåº«ï¼‰"""
    # ã‚«ãƒ†ã‚´ãƒªä¸­ãŒï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰ã§ãªã„
    shukko = df[~df['ã‚«ãƒ†ã‚´ãƒªä¸­'].str.contains('ï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰', na=False)].copy()
    
    # æ•°é‡è¨ˆç®—: å…¥åº«äºˆå®šæ•° * -1
    shukko['æ•°é‡'] = shukko['å…¥åº«äºˆå®šæ•°'] * -1
    shukko['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'] = shukko['ç§»å‹•å…ƒå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰']
    shukko['å–æ¬¡åº—å'] = shukko['ç§»å‹•å…ƒå–æ¬¡åº—å']
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = shukko.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'ç§»å‹•å…ƒäº‹æ¥­CD', 'ç§»å‹•å…ƒä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={
        'ç§»å‹•å…ƒäº‹æ¥­CD': 'äº‹æ¥­CD',
        'ç§»å‹•å…ƒä¿ç®¡å ´æ‰€CD': 'ä¿ç®¡å ´æ‰€CD',
        'æ•°é‡': 'å¤‰å‹•æ•°'
    })
    
    # ä¿ç®¡å ´æ‰€CDãŒç©ºã§ãªã„
    result = result[result['ä¿ç®¡å ´æ‰€CD'] != '']
    
    return result

def process_ido_nyuko(df: pd.DataFrame) -> pd.DataFrame:
    """ç§»å‹•ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¥åº«ï¼‰"""
    # ã‚«ãƒ†ã‚´ãƒªä¸­ãŒï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰ã§ãªã„
    nyuko = df[~df['ã‚«ãƒ†ã‚´ãƒªä¸­'].str.contains('ï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰', na=False)].copy()
    
    # æ•°é‡è¨ˆç®—: å…¥åº«äºˆå®šæ•° * 1
    nyuko['æ•°é‡'] = nyuko['å…¥åº«äºˆå®šæ•°'] * 1
    nyuko['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'] = nyuko['ç§»å‹•å…ˆå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰']
    nyuko['å–æ¬¡åº—å'] = nyuko['ç§»å‹•å…ˆå–æ¬¡åº—å']
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = nyuko.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'ç§»å‹•å…ˆäº‹æ¥­CD', 'ç§»å‹•å…ˆä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={
        'ç§»å‹•å…ˆäº‹æ¥­CD': 'äº‹æ¥­CD',
        'ç§»å‹•å…ˆä¿ç®¡å ´æ‰€CD': 'ä¿ç®¡å ´æ‰€CD',
        'æ•°é‡': 'å¤‰å‹•æ•°'
    })
    
    # ä¿ç®¡å ´æ‰€CDãŒç©ºã§ãªã„
    result = result[result['ä¿ç®¡å ´æ‰€CD'] != '']
    
    return result

def process_uri_data(df: pd.DataFrame, masters: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨å‡¦ç†ï¼ˆPower Queryæº–æ‹ ï¼‰
    """
    if df is None or df.empty:
        return pd.DataFrame()
    
    # ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆLeft Outer Joinï¼‰
    if '857001' in masters:
        df = df.merge(masters['857001'], on='å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='left')
    
    # å•†å“æ§‹æˆãƒã‚¹ã‚¿ã¨ã®ãƒãƒ¼ã‚¸ã¯çœç•¥ï¼ˆãƒã‚¹ã‚¿ãŒãªã„å ´åˆï¼‰
    # å•†å“ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ç”¨
    df['å•†å“ã‚³ãƒ¼ãƒ‰bk'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    
    # ãƒã‚¹ã‚¿857002ã¨ãƒãƒ¼ã‚¸
    if '857002' in masters:
        # å‹ã‚’çµ±ä¸€ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰
        df['äº‹æ¥­CD'] = df['äº‹æ¥­CD'].astype(str)
        master_857002 = masters['857002'].copy()
        master_857002['äº‹æ¥­CD'] = master_857002['äº‹æ¥­CD'].astype(str)
        
        df = df.merge(master_857002, on=['äº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'], how='left', suffixes=('', '_master'))
        df['TMSå•†å“CD'] = df['TMSå•†å“CD'].fillna(df['å•†å“ã‚³ãƒ¼ãƒ‰'])
    else:
        df['TMSå•†å“CD'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    
    # æ•°é‡è¨ˆç®—: åç´ç¨®åˆ¥ãŒã€Œè²©å£²ã€ãªã‚‰-1ã€ãã‚Œä»¥å¤–ã¯1
    df['æ•°é‡'] = df['åç´ç¨®åˆ¥'].apply(lambda x: -1 if x == 'è²©å£²' else 1)
    
    return df

def process_uri_individual(df: pd.DataFrame) -> pd.DataFrame:
    """å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆå€‹ä½“æƒ…å ±ï¼‰"""
    # å¿…è¦ãªåˆ—ãŒç„¡ã„å ´åˆã¯ç©ºæ–‡å­—åˆ—åˆ—ã‚’ä½œã‚‹ï¼ˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ã¯åˆ—åãŒå­˜åœ¨ã—ãªã„ã“ã¨ãŒã‚ã‚‹ï¼‰
    needed = ['ãƒ¡ãƒ¼ã‚«ãƒ¼', 'æ¥­å‹™ç¨®åˆ¥', 'åº—èˆ—å', 'å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD']
    for col in needed:
        if col not in df.columns:
            df[col] = ''

    # ãƒ¡ãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹ ã‹ã¤ Apple Inc.-SBSã¨ï½¿ï¾Œï¾„ï¾Šï¾ï¾ï½¸ï½¾ï¾šï½¸ï½¼ï½®ï¾ã§ãªã„
    individual = df[
        (df['ãƒ¡ãƒ¼ã‚«ãƒ¼'].notna()) &
        (df['ãƒ¡ãƒ¼ã‚«ãƒ¼'] != '') &
        (df['ãƒ¡ãƒ¼ã‚«ãƒ¼'] != 'Apple Inc.-SBS') &
        (df['ãƒ¡ãƒ¼ã‚«ãƒ¼'] != 'ï½¿ï¾Œï¾„ï¾Šï¾ï¾ï½¸ï½¾ï¾šï½¸ï½¼ï½®ï¾')
    ].copy()
    
    # æ¥­å‹™ç¨®åˆ¥ã«ï¼µï¼³ï¼©ï¼­ã‚’å«ã¾ãªã„
    individual = individual[~individual['æ¥­å‹™ç¨®åˆ¥'].str.contains('ï¼µï¼³ï¼©ï¼­', na=False)]
    
    individual['å–æ¬¡åº—å'] = individual['åº—èˆ—å']

    # æ•°é‡åˆ—ãŒç„¡ã‘ã‚Œã°ä½œæˆï¼ˆé€šå¸¸ã¯ process_uri_data ã§ä½œæˆã•ã‚Œã‚‹ï¼‰
    if 'æ•°é‡' not in individual.columns:
        if 'åç´ç¨®åˆ¥' in individual.columns:
            individual['æ•°é‡'] = individual['åç´ç¨®åˆ¥'].apply(lambda x: -1 if x == 'è²©å£²' else 1)
        else:
            individual['æ•°é‡'] = 0

    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = individual.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    
    return result

def process_uri_sb_accessory(df: pd.DataFrame) -> pd.DataFrame:
    """å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆSBã‚¢ã‚¯ã‚»ã‚µãƒªï¼‰"""
    # å¿…è¦ãªåˆ—ãŒç„¡ã„å ´åˆã¯ç©ºæ–‡å­—åˆ—åˆ—ã‚’ä½œã‚‹
    if 'å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰' not in df.columns:
        df['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'] = ''
    if 'ãƒ¡ãƒ¼ã‚«ãƒ¼' not in df.columns:
        df['ãƒ¡ãƒ¼ã‚«ãƒ¼'] = ''

    # å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰ãŒTGã§å§‹ã¾ã‚‹
    sb_acc = df[df['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'].str.startswith('TG', na=False)].copy()
    
    # ãƒ¡ãƒ¼ã‚«ãƒ¼ãŒApple Inc.-SBSã¾ãŸã¯ï½¿ï¾Œï¾„ï¾Šï¾ï¾ï½¸ï½¾ï¾šï½¸ï½¼ï½®ï¾
    sb_acc = sb_acc[
        (sb_acc['ãƒ¡ãƒ¼ã‚«ãƒ¼'] == 'Apple Inc.-SBS') |
        (sb_acc['ãƒ¡ãƒ¼ã‚«ãƒ¼'] == 'ï½¿ï¾Œï¾„ï¾Šï¾ï¾ï½¸ï½¾ï¾šï½¸ï½¼ï½®ï¾')
    ]
    
    sb_acc['å–æ¬¡åº—å'] = sb_acc['åº—èˆ—å']

    # æ•°é‡åˆ—ãŒç„¡ã‘ã‚Œã°ä½œæˆ
    if 'æ•°é‡' not in sb_acc.columns:
        if 'åç´ç¨®åˆ¥' in sb_acc.columns:
            sb_acc['æ•°é‡'] = sb_acc['åç´ç¨®åˆ¥'].apply(lambda x: -1 if x == 'è²©å£²' else 1)
        else:
            sb_acc['æ•°é‡'] = 0

    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = sb_acc.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    
    return result

def process_uri_service(df: pd.DataFrame) -> pd.DataFrame:
    """å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µãƒ¼ãƒ“ã‚¹ï¼‰"""
    # å¿…è¦ãªåˆ—ãŒç„¡ã„å ´åˆã¯ç©ºæ–‡å­—åˆ—åˆ—ã‚’ä½œã‚‹
    if 'å•†å“åˆ†é¡' not in df.columns:
        df['å•†å“åˆ†é¡'] = ''
    if 'åº—èˆ—å' not in df.columns:
        df['åº—èˆ—å'] = ''

    # å•†å“åˆ†é¡ãŒã€Œã‚µãƒ¼ãƒ“ã‚¹ã€
    service = df[df['å•†å“åˆ†é¡'] == 'ã‚µãƒ¼ãƒ“ã‚¹'].copy()
    
    service['å–æ¬¡åº—å'] = service['åº—èˆ—å']

    # æ•°é‡åˆ—ãŒç„¡ã‘ã‚Œã°ä½œæˆ
    if 'æ•°é‡' not in service.columns:
        if 'åç´ç¨®åˆ¥' in service.columns:
            service['æ•°é‡'] = service['åç´ç¨®åˆ¥'].apply(lambda x: -1 if x == 'è²©å£²' else 1)
        else:
            service['æ•°é‡'] = 0

    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = service.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    
    return result

def process_tana_data(df: pd.DataFrame, masters: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    æ£šå¸ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨å‡¦ç†ï¼ˆPower Queryæº–æ‹ ï¼‰
    """
    if df is None or df.empty:
        return pd.DataFrame()
    
    # ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆInner Joinï¼‰
    if '857001' in masters:
        df = df.merge(masters['857001'], on='å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='inner')
    
    # å‹å¤‰æ›
    df['å—æ‰•å‰åœ¨åº«æ•°'] = pd.to_numeric(df['å—æ‰•å‰åœ¨åº«æ•°'], errors='coerce').fillna(0).astype(int)
    df['æ•°é‡'] = pd.to_numeric(df['æ•°é‡'], errors='coerce').fillna(0).astype(int)
    df['å—æ‰•å¾Œåœ¨åº«æ•°'] = pd.to_numeric(df['å—æ‰•å¾Œåœ¨åº«æ•°'], errors='coerce').fillna(0).astype(int)
    
    # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
    df = df.loc[:, ~df.columns.str.startswith('_')]
    df = df.loc[:, df.columns != '']
    
    # åº—èˆ—å€‰åº«åŒºåˆ†ã§ãƒ•ã‚£ãƒ«ã‚¿
    df = df[df['åº—èˆ—å€‰åº«åŒºåˆ†'] == '1']
    
    # ãƒã‚¹ã‚¿857002ã¨ãƒãƒ¼ã‚¸
    if '857002' in masters:
        # å‹ã‚’çµ±ä¸€ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰
        df['äº‹æ¥­CD'] = df['äº‹æ¥­CD'].astype(str)
        master_857002 = masters['857002'].copy()
        master_857002['äº‹æ¥­CD'] = master_857002['äº‹æ¥­CD'].astype(str)
        
        df = df.merge(master_857002, on=['äº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'], how='left', suffixes=('', '_master'))
        df['TMSå•†å“CD'] = df['TMSå•†å“CD'].fillna(df['å•†å“ã‚³ãƒ¼ãƒ‰'])
    else:
        df['TMSå•†å“CD'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    
    # é™¤å¤–å•†å“ã‚³ãƒ¼ãƒ‰
    exclude_codes = ['ZUA292', 'ZUA34Q', 'ZUA34R', 'ZUA34S', 'ZUA34T', 'ZUA34U', 'ZUA34V', 'ZUA34W']
    for code in exclude_codes:
        df = df[~df['å•†å“ã‚³ãƒ¼ãƒ‰'].str.contains(code, na=False)]
    
    return df

def process_tana_grouped(df: pd.DataFrame) -> pd.DataFrame:
    """æ£šå¸ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰"""
    # ã‚«ãƒ†ã‚´ãƒªä¸­ãŒï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰ã§ãªã„
    grouped = df[~df['ã‚«ãƒ†ã‚´ãƒªä¸­'].str.contains('ï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰', na=False)].copy()
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = grouped.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    
    return result

def combine_all_data(shiire_ind, shiire_acc, ido_shukko, ido_nyuko, 
                     uri_ind, uri_sb, uri_service, tana_grouped) -> pd.DataFrame:
    """
    å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦TMSå•†å“CDã§é›†è¨ˆï¼ˆGINIEPOSå¤‰å‹•æ•°ï¼‰
    """
    all_dfs = []
    
    for df in [shiire_ind, shiire_acc, ido_shukko, ido_nyuko, uri_ind, uri_sb, uri_service, tana_grouped]:
        if df is not None and not df.empty:
            all_dfs.append(df)
    
    if not all_dfs:
        return pd.DataFrame()
    
    # å…¨ã¦çµåˆ
    combined = pd.concat(all_dfs, ignore_index=True)
    
    # TMSå•†å“CDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦åˆè¨ˆ
    result = combined.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'TMSå•†å“CD'],
        dropna=False
    )['å¤‰å‹•æ•°'].sum().reset_index()
    
    # ã‚½ãƒ¼ãƒˆ
    result = result.sort_values('å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰').reset_index(drop=True)
    
    return result

def compare_with_current_inventory(giniepos_df: pd.DataFrame, current_df: pd.DataFrame) -> pd.DataFrame:
    """
    GINIEPOSå¤‰å‹•æ•°ã¨ç¾åœ¨åº«ç…§ä¼šã‚’æ¯”è¼ƒï¼ˆåˆ¤å®šçµæœï¼‰
    """
    if giniepos_df is None or giniepos_df.empty or current_df is None or current_df.empty:
        return pd.DataFrame()
    
    # ç¾åœ¨åº«ç…§ä¼šã‹ã‚‰å¿…è¦ãªåˆ—ã®ã¿å–å¾—
    current_summary = current_df[['ä¿ç®¡å ´æ‰€CD', 'äº‹æ¥­CD', 'å•†å“CD', 'å®Ÿåœ¨åº«æ•°é‡']].copy()
    
    # å‹ã‚’çµ±ä¸€ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰
    giniepos_df['äº‹æ¥­CD'] = giniepos_df['äº‹æ¥­CD'].astype(str)
    current_summary['äº‹æ¥­CD'] = current_summary['äº‹æ¥­CD'].astype(str)
    current_summary['ä¿ç®¡å ´æ‰€CD'] = current_summary['ä¿ç®¡å ´æ‰€CD'].astype(str)
    current_summary['å•†å“CD'] = current_summary['å•†å“CD'].astype(str)
    giniepos_df['ä¿ç®¡å ´æ‰€CD'] = giniepos_df['ä¿ç®¡å ´æ‰€CD'].astype(str)
    giniepos_df['TMSå•†å“CD'] = giniepos_df['TMSå•†å“CD'].astype(str)
    
    # ãƒãƒ¼ã‚¸ï¼ˆLeft Outer Joinï¼‰
    result = giniepos_df.merge(
        current_summary,
        left_on=['ä¿ç®¡å ´æ‰€CD', 'äº‹æ¥­CD', 'TMSå•†å“CD'],
        right_on=['ä¿ç®¡å ´æ‰€CD', 'äº‹æ¥­CD', 'å•†å“CD'],
        how='left'
    )
    
    # CLå®Ÿåœ¨åº«æ•°ã¨å‘¼ã¶
    result = result.rename(columns={'å®Ÿåœ¨åº«æ•°é‡': 'CLå®Ÿåœ¨åº«æ•°'})
    
    # nullã¯0ã«ç½®æ›
    result['CLå®Ÿåœ¨åº«æ•°'] = result['CLå®Ÿåœ¨åº«æ•°'].fillna(0)
    
    # åˆ¤å®š = å¤‰å‹•æ•° + CLå®Ÿåœ¨åº«æ•°
    result['åˆ¤å®š'] = result['å¤‰å‹•æ•°'] + result['CLå®Ÿåœ¨åº«æ•°']
    
    # ãƒ‘ã‚¹ãƒãƒç­‰ã‚’é™¤å¤–ï¼ˆåˆ¤å®šå‰ï¼‰
    result = result[~result['TMSå•†å“CD'].str.contains('BB-RQ8POU1740', na=False)]
    result = result[~result['TMSå•†å“CD'].str.contains('ZUA292', na=False)]
    
    # åœ¨åº«ä¸è¶³ã®åˆ¤å®šï¼ˆåˆ¤å®š < 0ï¼‰
    result = result[result['åˆ¤å®š'] < 0]
    
    # POS-ã€Z00014ã‚’é™¤å¤–ï¼ˆåœ¨åº«ä¸è¶³æŠ½å‡ºå¾Œï¼‰
    result = result[~result['TMSå•†å“CD'].str.contains('Z00014', na=False)]
    result = result[~result['TMSå•†å“CD'].str.contains('POS-', na=False)]
    
    return result

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³
st.header("1ï¸âƒ£ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

# å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®é¸æŠ
mode = st.selectbox("å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„", ["Full (8 files)", "Sales only (single sales file)"])

col1, col2 = st.columns([4, 1])

with col1:
    if mode == "Full (8 files)":
        st.info("ğŸ“ å¿…è¦ãª8ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã¾ã¨ã‚ã¦ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„")
    else:
        st.info("ğŸ“ å£²ä¸Šãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã§ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚å£²ä¸Šãƒ•ã‚¡ã‚¤ãƒ«ã¨ç¾åœ¨åº«ï¼ˆExcelï¼‰ã€å¿…è¦ãªãƒã‚¹ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")

with col2:
    if st.button("ğŸ”„ ã‚¯ãƒªã‚¢", key='clear_btn', help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢"):
        for k in list(st.session_state.keys()):
            del st.session_state[k]
        safe_rerun()

if mode == "Full (8 files)":
    uploaded_files = st.file_uploader(
        "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã¾ãŸã¯ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—",
        type=['csv', 'xlsx', 'xls'],
        accept_multiple_files=True,
        help="åœ¨åº«å¤‰å‹•ãƒ‡ãƒ¼ã‚¿4ãƒ•ã‚¡ã‚¤ãƒ« + ç¾åœ¨åº«ç…§ä¼š + ãƒã‚¹ã‚¿3ãƒ•ã‚¡ã‚¤ãƒ« = è¨ˆ8ãƒ•ã‚¡ã‚¤ãƒ«"
    )
else:
    # Sales-only mode: å˜ä¸€ã®å£²ä¸Šãƒ•ã‚¡ã‚¤ãƒ«ã€ç¾åœ¨åº«ã€ãƒã‚¹ã‚¿ï¼ˆä»»æ„ï¼‰ã‚’å—ã‘å–ã‚‹
    sales_file = st.file_uploader(
        "å£²ä¸Šãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆCSVï¼‰",
        type=['csv'],
        accept_multiple_files=False,
        key='sales_only'
    )
    current_file = st.file_uploader(
        "ç¾åœ¨åº«ç…§ä¼šãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆExcelï¼‰",
        type=['xlsx', 'xls'],
        accept_multiple_files=False,
        key='current_only'
    )
    st.markdown("---")
    st.markdown("**å¿…è¦ã«å¿œã˜ã¦ãƒã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ857001, 857002, 857003ï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**")
    master_857001_file = st.file_uploader("ãƒã‚¹ã‚¿857001 (å–æ¬¡åº—)", type=['csv'], key='m857001')
    master_857002_file = st.file_uploader("ãƒã‚¹ã‚¿857002 (å•†å“)", type=['csv'], key='m857002')
    master_857003_file = st.file_uploader("ãƒã‚¹ã‚¿857003 (ä»•å…¥å…ˆ)", type=['csv'], key='m857003')

# ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ¯ã‚Šåˆ†ã‘
shiire_file = None
ido_file = None
uri_file = None
tana_file = None
current_file = None
master_857001_file = None
master_857002_file = None
master_857003_file = None

if uploaded_files:
    st.subheader("ğŸ“‹ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«")
    
    for file in uploaded_files:
        filename = file.name
        
        if 'SHI' in filename.upper():
            shiire_file = file
            st.success(f"âœ… ä»•å…¥ãƒ‡ãƒ¼ã‚¿: {filename}")
        elif 'IDO' in filename.upper():
            ido_file = file
            st.success(f"âœ… ç§»å‹•ãƒ‡ãƒ¼ã‚¿: {filename}")
        elif 'URI' in filename.upper():
            uri_file = file
            st.success(f"âœ… å£²ä¸Šãƒ‡ãƒ¼ã‚¿: {filename}")
        elif 'TNA' in filename.upper():
            tana_file = file
            st.success(f"âœ… æ£šå¸ãƒ‡ãƒ¼ã‚¿: {filename}")
        elif 'ç¾åœ¨åº«' in filename or 'ZAIKO' in filename.upper():
            current_file = file
            st.success(f"âœ… ç¾åœ¨åº«ç…§ä¼š: {filename}")
        elif '857001' in filename:
            master_857001_file = file
            st.success(f"âœ… ãƒã‚¹ã‚¿857001ï¼ˆå–æ¬¡åº—ï¼‰: {filename}")
        elif '857002' in filename:
            master_857002_file = file
            st.success(f"âœ… ãƒã‚¹ã‚¿857002ï¼ˆå•†å“ã‚³ãƒ¼ãƒ‰ï¼‰: {filename}")
        elif '857003' in filename:
            master_857003_file = file
            st.success(f"âœ… ãƒã‚¹ã‚¿857003ï¼ˆä»•å…¥å…ˆï¼‰: {filename}")
        else:
            st.warning(f"âš ï¸ ä¸æ˜ãªãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
    total_files = sum([
        shiire_file is not None,
        ido_file is not None,
        uri_file is not None,
        tana_file is not None,
        current_file is not None,
        master_857001_file is not None,
        master_857002_file is not None,
        master_857003_file is not None
    ])
    
    if total_files < 8:
        st.warning(f"âš ï¸ {total_files}/8ãƒ•ã‚¡ã‚¤ãƒ«ãŒèªè­˜ã•ã‚Œã¾ã—ãŸã€‚å…¨8ãƒ•ã‚¡ã‚¤ãƒ«å¿…è¦ã§ã™ã€‚")
    else:
        st.success("âœ… å…¨8ãƒ•ã‚¡ã‚¤ãƒ«ãŒæƒã„ã¾ã—ãŸï¼")

st.markdown("---")

# å‡¦ç†å®Ÿè¡Œãƒœã‚¿ãƒ³
if st.button("ğŸš€ åœ¨åº«ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ", type="primary", use_container_width=True):
    if not all([shiire_file, ido_file, uri_file, tana_file, current_file]):
        st.error("âš ï¸ åœ¨åº«å¤‰å‹•ãƒ‡ãƒ¼ã‚¿ã¨ç¾åœ¨åº«ç…§ä¼šãƒ•ã‚¡ã‚¤ãƒ«ã¯å¿…é ˆã§ã™")
    else:
        with st.spinner("å‡¦ç†ä¸­..."):
            try:
                # ãƒã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
                st.info("ğŸ“š ãƒã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
                masters = load_master_files(master_857001_file, master_857002_file, master_857003_file)
                
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆLFæ”¹è¡Œã€Shift-JISï¼‰
                st.info("ğŸ“‚ åœ¨åº«å¤‰å‹•ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
                shiire_df = load_csv_with_encoding(shiire_file, use_lf=True, encoding='cp932')
                ido_df = load_csv_with_encoding(ido_file, use_lf=True, encoding='cp932')
                uri_df = load_csv_with_encoding(uri_file, use_lf=True, encoding='cp932')
                if uri_df is None:
                    uri_df = pd.DataFrame()
                tana_df = load_csv_with_encoding(tana_file, use_lf=True, encoding='cp932')
                
                # ç¾åœ¨åº«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
                st.info("ğŸ“Š ç¾åœ¨åº«ç…§ä¼šèª­ã¿è¾¼ã¿ä¸­...")
                current_df = pd.read_excel(current_file)
                st.success(f"âœ… ç¾åœ¨åº«ç…§ä¼š: {len(current_df)}è¡Œ")
                
                # ä»•å…¥ãƒ‡ãƒ¼ã‚¿å‡¦ç†
                st.info("ğŸ”„ ä»•å…¥ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­...")
                shiire_processed = process_shiire_data(shiire_df, masters)
                shiire_individual = process_shiire_individual(shiire_processed)
                shiire_accessory = process_shiire_accessory(shiire_processed)
                st.success(f"âœ… ä»•å…¥ï¼ˆå€‹ä½“ï¼‰: {len(shiire_individual)}è¡Œã€ä»•å…¥ï¼ˆã‚¢ã‚¯ã‚»ã‚µãƒªï¼‰: {len(shiire_accessory)}è¡Œ")
                
                # ç§»å‹•ãƒ‡ãƒ¼ã‚¿å‡¦ç†
                st.info("ğŸ”„ ç§»å‹•ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­...")
                ido_processed = process_ido_data(ido_df, masters)
                ido_shukko = process_ido_shukko(ido_processed)
                ido_nyuko = process_ido_nyuko(ido_processed)
                st.success(f"âœ… ç§»å‹•ï¼ˆå‡ºåº«ï¼‰: {len(ido_shukko)}è¡Œã€ç§»å‹•ï¼ˆå…¥åº«ï¼‰: {len(ido_nyuko)}è¡Œ")
                
                # å£²ä¸Šãƒ‡ãƒ¼ã‚¿å‡¦ç†
                st.info("ğŸ”„ å£²ä¸Šãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­...")

                # 1) main.py ç›¸å½“ã® CSV ãƒ¬ãƒ™ãƒ«ã®ãƒã‚§ãƒƒã‚¯ã‚’å…ˆã«å®Ÿè¡Œï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ¤œæŸ»ï¼‰
                try:
                    raw_bytes = uri_file.getvalue()
                    text = raw_bytes.decode('cp932')
                except Exception:
                    text = None

                if text:
                    try:
                        err_flag, err_details, total_records, total_physical_lines, date_summary = check_and_analyze(text)
                        if err_flag:
                            st.error("âŒ å£²ä¸Šãƒ•ã‚¡ã‚¤ãƒ«ã« NG æ¡ä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
                            st.write(f"NGä»¶æ•°: {len(err_details)} ä»¶")
                            err_csv = build_error_csv_bytes(err_details)
                            st.download_button("NGè¡Œä¸€è¦§ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (UTF-8)", data=err_csv, file_name=f"{uri_file.name}_ng.csv")
                            # æ—¥ä»˜æŒ‡æ‘˜ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½
                            ds_bytes = build_date_issue_csv_bytes(date_summary.issues)
                            st.download_button("æ—¥ä»˜ãƒã‚§ãƒƒã‚¯æŒ‡æ‘˜ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (UTF-8)", data=ds_bytes, file_name=f"{uri_file.name}_date_issues.csv")
                            st.stop()
                        else:
                            # æ—¥ä»˜ãƒã‚§ãƒƒã‚¯ã®è­¦å‘Šãªã©ã‚’è¡¨ç¤ºï¼ˆã‚ã‚Œã°ï¼‰
                            if date_summary.issues:
                                st.warning(f"æ—¥ä»˜ãƒã‚§ãƒƒã‚¯ã§æŒ‡æ‘˜ãŒã‚ã‚Šã¾ã™ï¼ˆ{len(date_summary.issues)} ä»¶ï¼‰ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                                ds_bytes = build_date_issue_csv_bytes(date_summary.issues)
                                st.download_button("æ—¥ä»˜ãƒã‚§ãƒƒã‚¯æŒ‡æ‘˜ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (UTF-8)", data=ds_bytes, file_name=f"{uri_file.name}_date_issues.csv")
                    except Exception as e:
                        st.warning(f"å£²ä¸Šãƒ•ã‚¡ã‚¤ãƒ«ã®äº‹å‰ãƒã‚§ãƒƒã‚¯ã§ä¾‹å¤–: {e}")

                # 2) æ—¢å­˜ã®å‰å‡¦ç†: AGåˆ—å‰Šé™¤ã¨ãƒ‰ã‚³ãƒ¢ã‚·ãƒ§ãƒƒãƒ—æŠ½å‡º
                try:
                    before_rows = len(uri_df)
                    uri_df = drop_ag_column(uri_df)
                    kept_df, omitted_df = split_docomo_shop_rows(uri_df)
                    kept_rows = len(kept_df)
                    omitted_rows = len(omitted_df)
                    st.info(f"ğŸ” å£²ä¸Šå‰å‡¦ç†: {before_rows}è¡Œ -> ãƒ‰ã‚³ãƒ¢ã‚·ãƒ§ãƒƒãƒ—æŠ½å‡º {kept_rows}è¡Œ (é™¤å¤– {omitted_rows}è¡Œ)")
                    uri_df = kept_df
                except Exception as e:
                    st.warning(f"å£²ä¸Šå‰å‡¦ç†ã§æ³¨æ„: {e}")

                uri_processed = process_uri_data(uri_df, masters)
                uri_individual = process_uri_individual(uri_processed)
                uri_sb_accessory = process_uri_sb_accessory(uri_processed)
                uri_service = process_uri_service(uri_processed)
                st.success(f"âœ… å£²ä¸Šï¼ˆå€‹ä½“ï¼‰: {len(uri_individual)}è¡Œã€å£²ä¸Šï¼ˆSBã‚¢ã‚¯ã‚»ã‚µãƒªï¼‰: {len(uri_sb_accessory)}è¡Œã€å£²ä¸Šï¼ˆã‚µãƒ¼ãƒ“ã‚¹ï¼‰: {len(uri_service)}è¡Œ")
                
                # æ£šå¸ãƒ‡ãƒ¼ã‚¿å‡¦ç†
                st.info("ğŸ”„ æ£šå¸ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­...")
                tana_processed = process_tana_data(tana_df, masters)
                tana_grouped = process_tana_grouped(tana_processed)
                st.success(f"âœ… æ£šå¸: {len(tana_grouped)}è¡Œ")
                
                # å…¨ãƒ‡ãƒ¼ã‚¿çµåˆ
                st.info("ğŸ”— ãƒ‡ãƒ¼ã‚¿çµåˆãƒ»é›†è¨ˆä¸­...")
                giniepos_hendo = combine_all_data(
                    shiire_individual, shiire_accessory,
                    ido_shukko, ido_nyuko,
                    uri_individual, uri_sb_accessory, uri_service,
                    tana_grouped
                )
                st.success(f"âœ… GINIEPOSå¤‰å‹•æ•°: {len(giniepos_hendo)}è¡Œ")
                
                # ç¾åœ¨åº«ã¨ã®æ¯”è¼ƒ
                st.info("ğŸ” åœ¨åº«éä¸è¶³ãƒã‚§ãƒƒã‚¯ä¸­...")
                result_df = compare_with_current_inventory(giniepos_hendo, current_df)
                
                # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                st.session_state.processed_data = result_df
                
                st.success("âœ… å‡¦ç†å®Œäº†ï¼")
                
            except Exception as e:
                st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                st.exception(e)

# çµæœè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³
if st.session_state.processed_data is not None and not st.session_state.processed_data.empty:
    st.markdown("---")
    st.header("2ï¸âƒ£ åœ¨åº«ä¸è¶³çµæœ")
    
    result_df = st.session_state.processed_data
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("åœ¨åº«ä¸è¶³ä»¶æ•°", f"{len(result_df)}ä»¶")
    with col2:
        total_hendo = result_df['å¤‰å‹•æ•°'].sum()
        st.metric("å¤‰å‹•æ•°åˆè¨ˆ", f"{total_hendo:,}")
    with col3:
        total_cl = result_df['CLå®Ÿåœ¨åº«æ•°'].sum()
        st.metric("CLå®Ÿåœ¨åº«åˆè¨ˆ", f"{int(total_cl):,}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    st.subheader("ğŸ“‹ è©³ç´°ãƒªã‚¹ãƒˆ")
    
    display_cols = ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'TMSå•†å“CD', 'å¤‰å‹•æ•°', 'CLå®Ÿåœ¨åº«æ•°', 'åˆ¤å®š']
    available_cols = [col for col in display_cols if col in result_df.columns]
    
    st.dataframe(
        result_df[available_cols].sort_values('åˆ¤å®š'),
        use_container_width=True,
        height=400
    )
    
    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
    csv = result_df[available_cols].to_csv(index=False, encoding='cp932')
    st.download_button(
        label="ğŸ“¥ çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="åœ¨åº«ä¸è¶³çµæœ.csv",
        mime="text/csv",
        use_container_width=True
    )

elif st.session_state.processed_data is not None:
    st.success("âœ… åœ¨åº«ä¸è¶³ã¯ã‚ã‚Šã¾ã›ã‚“ï¼")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("**åœ¨åº«ä¸è¶³ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ** | Python + Streamlitç‰ˆ | Power Queryå®Œå…¨æº–æ‹ ")
