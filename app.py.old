
import streamlit as st
import pandas as pd
import io
import unicodedata
from typing import Dict, List, Optional, Tuple
import csv
import hashlib
import re
from dataclasses import dataclass
from datetime import datetime, date
from io import StringIO

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="äº‹å‰ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“¦",
    layout="wide"
)
st.title("ğŸ§ª äº‹å‰ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("---")

# ----------------------------
# Utilities
# ----------------------------
def normalize_key_series(s: pd.Series) -> pd.Series:
    """PowerQueryã®Trimç›¸å½“: å‰å¾Œç©ºç™½ï¼ˆå…¨è§’å«ã‚€ï¼‰ãƒ»BOMç­‰ã‚’é™¤å»ã—ã€çªåˆã‚­ãƒ¼ã®ãƒ–ãƒ¬ã‚’é˜²ãã€‚"""
    out = s.astype(str)
    out = out.str.replace("\ufeff", "", regex=False)
    out = out.str.replace("ã€€", " ", regex=False)  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹â†’åŠè§’
    out = out.str.strip()
    # æ–‡å­—åˆ—åŒ–ã§æ··å…¥ã™ã‚‹è¡¨ç¾ã‚’ç©ºã«å¯„ã›ã‚‹
    out = out.replace({"nan": "", "None": "", "NaN": ""})
    return out


# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None

# file_uploader reset key
if 'uploader_version' not in st.session_state:
    st.session_state.uploader_version = 0

def safe_rerun() -> None:
    """Rerun the Streamlit script in a way compatible with multiple Streamlit versions."""
    # Newer Streamlit
    if hasattr(st, "rerun"):
        try:
            st.rerun()
            return
        except Exception:
            pass
    # Older Streamlit
    if hasattr(st, "experimental_rerun"):
        try:
            st.experimental_rerun()
            return
        except Exception:
            pass
    # Fallback: raise the internal RerunException (very old versions)
    try:
        from streamlit.runtime.scriptrunner.script_runner import RerunException
        raise RerunException()
    except Exception:
        # As last resort, stop to prevent further UI actions
        try:
            st.stop()
        except Exception:
            return

def load_csv_with_encoding(file, use_lf=True, encoding='cp932') -> pd.DataFrame:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã‚’æŒ‡å®šï¼‰
    """
    try:
        content = file.read()
        decoded_content = content.decode(encoding)
        if use_lf:
            df = pd.read_csv(io.StringIO(decoded_content), lineterminator='\n')
        else:
            df = pd.read_csv(io.StringIO(decoded_content))
        return df
    except Exception as e:
        st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def load_master_files(master_857001, master_857002, master_857003) -> Dict[str, pd.DataFrame]:
    """
    3ã¤ã®ãƒã‚¹ã‚¿CSVã‚’èª­ã¿è¾¼ã‚€ï¼ˆUTF-8 / UTF-8-SIGæƒ³å®šï¼‰
    åˆ—åã®ä¸–ä»£å·®ï¼ˆä¾‹: å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰401 vs å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤01 ãªã©ï¼‰ã‚’å¸åã™ã‚‹ã€‚
    PowerQueryã§è¡Œã£ã¦ã„ãŸTrimç›¸å½“ï¼ˆå‰å¾Œç©ºç™½é™¤å»ãƒ»å…¨è§’ç©ºç™½é™¤å»ï¼‰ã‚‚ã“ã“ã§å®Ÿæ–½ã™ã‚‹ã€‚
    """
    def _nfkc(s: str) -> str:
        return unicodedata.normalize("NFKC", s).replace("\ufeff", "").strip()

    def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        df.columns = [_nfkc(str(c)) for c in df.columns]
        return df

    def normalize_text_series(s: pd.Series) -> pd.Series:
        # Trim + å…¨è§’ç©ºç™½é™¤å» + NFKC
        return (
            s.astype(str)
             .map(lambda x: _nfkc(x).replace(" ", "").replace("ã€€", ""))  # åŠè§’/å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹é™¤å»
        )

    def pick_columns(df: pd.DataFrame, mapping: Dict[str, List[str]]) -> pd.DataFrame:
        colmap: Dict[str, str] = {}
        for dst, candidates in mapping.items():
            found = None
            for c in candidates:
                if c in df.columns:
                    found = c
                    break
            if found is None:
                raise KeyError(
                    f"ãƒã‚¹ã‚¿åˆ— '{dst}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å€™è£œ={candidates} / å®Ÿåˆ—={df.columns.tolist()}"
                )
            colmap[dst] = found
        out = df[[colmap[k] for k in mapping.keys()]].copy()
        out.columns = list(mapping.keys())
        return out

    masters: Dict[str, pd.DataFrame] = {}

    # 857001ï¼ˆå–æ¬¡åº—â†’äº‹æ¥­/ä¿ç®¡å ´æ‰€ãªã©ï¼‰
    if master_857001:
        df = load_csv_with_encoding(master_857001, use_lf=False, encoding="utf-8-sig")
        if df is not None and not df.empty:
            df = normalize_columns(df)
            df = pick_columns(df, {
                "å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰": ["å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰401", "å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤01", "å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰01"],
                "åº—èˆ—å€‰åº«åŒºåˆ†": ["ã‚³ãƒ¼ãƒ‰1", "ã‚³ãƒ¼ãƒ‰å€¤1"],
                "äº‹æ¥­CDï¼¢ï¼«": ["ã‚³ãƒ¼ãƒ‰2", "ã‚³ãƒ¼ãƒ‰å€¤2"],
                "ä¿ç®¡å ´æ‰€CD": ["ã‚³ãƒ¼ãƒ‰4", "ã‚³ãƒ¼ãƒ‰å€¤4"],
            })

            # Trimç›¸å½“
            for c in ["å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰", "åº—èˆ—å€‰åº«åŒºåˆ†", "äº‹æ¥­CDï¼¢ï¼«", "ä¿ç®¡å ´æ‰€CD"]:
                df[c] = normalize_text_series(df[c])

            # äº‹æ¥­CDã®æ±ºå®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ç¶­æŒï¼‰
            df["äº‹æ¥­CD"] = df["å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰"].apply(lambda x: "13000" if str(x).startswith("TG") else "15000")
            df = df.drop_duplicates(subset=["å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰"])
            masters["857001"] = df

    # 857002ï¼ˆå•†å“ã‚³ãƒ¼ãƒ‰â†’TMSå•†å“CD å¤‰æ›ï¼‰
    if master_857002:
        df = load_csv_with_encoding(master_857002, use_lf=False, encoding="utf-8-sig")
        if df is not None and not df.empty:
            df = normalize_columns(df)
            df = pick_columns(df, {
                "å•†å“ã‚³ãƒ¼ãƒ‰": ["å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰401", "å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤01", "å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰01"],
                "äº‹æ¥­CD": ["å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰402", "å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰å€¤02", "å¤‰æ›å‰ã‚³ãƒ¼ãƒ‰02"],
                "TMSå•†å“CD": ["ã‚³ãƒ¼ãƒ‰1", "ã‚³ãƒ¼ãƒ‰å€¤1"],
            })

            for c in ["å•†å“ã‚³ãƒ¼ãƒ‰", "äº‹æ¥­CD", "TMSå•†å“CD"]:
                df[c] = normalize_text_series(df[c])

            df = df.drop_duplicates(subset=["å•†å“ã‚³ãƒ¼ãƒ‰", "äº‹æ¥­CD"])
            masters["857002"] = df

    # 857003ï¼ˆãã®ä»–ã‚³ãƒ¼ãƒ‰å¤‰æ›ç­‰ï¼šåŠ å·¥ã›ãšã«ä¿æŒï¼‰
    if master_857003:
        df = load_csv_with_encoding(master_857003, use_lf=False, encoding="utf-8-sig")
        if df is not None and not df.empty:
            df = normalize_columns(df)
            masters["857003"] = df

    return masters
def drop_ag_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    Excelã®AGåˆ— = 33åˆ—ç›®ï¼ˆ1-basedï¼‰= index 32ï¼ˆ0-basedï¼‰
    å­˜åœ¨ã™ã‚Œã°å‰Šé™¤ã™ã‚‹ã€‚CSVã§ã‚‚åˆ—æ•°ãŒå¤šã‘ã‚Œã°åŒæ§˜ã«å‹•ä½œã™ã‚‹ã€‚
    """
    ag_index = 32
    if df.shape[1] <= ag_index:
        return df
    return df.drop(df.columns[ag_index], axis=1)

def normalize_text(s: str) -> str:
    """åŠè§’ãƒ»å…¨è§’ã®æºã‚Œã‚’å¸åã™ã‚‹ãŸã‚ã«æ­£è¦åŒ–ã™ã‚‹ï¼ˆNFKCï¼‰"""
    return unicodedata.normalize("NFKC", s)

def split_docomo_shop_rows(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Dåˆ—ï¼ˆ4åˆ—ç›®ï¼‰ã«ã€Œãƒ‰ã‚³ãƒ¢ã‚·ãƒ§ãƒƒãƒ—ï¼ˆåŠè§’å…¨è§’å•ã‚ãšï¼‰ã€ã‚’å«ã‚€è¡Œã‚’æ®‹ã—ã€
    ãã‚Œä»¥å¤–ã‚’ omitï¼ˆé™¤å¤–ï¼‰ã¨ã—ã¦åˆ†é›¢ã™ã‚‹ã€‚
    å®‰å…¨ã®ãŸã‚ã€å¯¾è±¡åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™ã€‚
    """
    col_index = 3  # Dåˆ—ï¼ˆ0-basedï¼‰
    if df.shape[1] <= col_index:
        # å¯¾è±¡åˆ—ãŒãªã„ã®ã§åˆ†é›¢ã¯è¡Œã‚ãªã„
        return df.copy(), pd.DataFrame()
    keyword = normalize_text("ãƒ‰ã‚³ãƒ¢ã‚·ãƒ§ãƒƒãƒ—")
    series = (
        df.iloc[:, col_index]
        .astype("string")
        .fillna("")
        .map(normalize_text)
    )
    mask = series.str.contains(keyword, na=False)
    kept_df = df[mask].copy()
    omitted_df = df[~mask].copy()
    return kept_df, omitted_df

# --- main.py ã®ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½å–ã‚Šè¾¼ã¿ ---
TARGET_COL_25 = 24
TARGET_COL_38 = 37
DATE_COL_9 = 8
DATE_COL_17 = 16
DATE_TIME_RE = re.compile(r"^\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}$")

@dataclass
class ErrorDetail:
    row: int
    store_name: str
    slip_number: str
    col_38: str

@dataclass
class DateIssue:
    record_no: int
    start_physical_line: int
    severity: str
    issue_type: str
    col9: str
    col17: str
    note: str

@dataclass
class DateSummary:
    total_checked_cells: int
    count_col9_ok: int
    count_error: int
    issues: List[DateIssue]

def csv_reader_from_text(csv_text: str):
    return csv.reader(StringIO(csv_text, newline=""))



def load_csv_from_text(csv_text: str) -> pd.DataFrame:
    """Read CSV content (already decoded to text) into a DataFrame.
    Keep all columns as strings to avoid dtype surprises.
    """
    return pd.read_csv(StringIO(csv_text), dtype=str, keep_default_na=False)

def load_current_inventory_excel(uploaded_file) -> pd.DataFrame:
    """ç¾åœ¨åº«ç…§ä¼šï¼ˆå•†å“åˆ¥ï¼‰Excelã‚’DataFrameã¨ã—ã¦èª­ã¿è¾¼ã‚€ã€‚

    - 1æšç›®ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€ï¼ˆPowerQueryé‹ç”¨ã¨åŒã˜å‰æï¼‰
    - åˆ—åã®å‰å¾Œç©ºç™½/BOM/å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»
    - ä»¥é™ã®å‡¦ç†ï¼ˆcompare_with_current_inventoryï¼‰ãŒæœŸå¾…ã™ã‚‹åˆ—å
      ['ä¿ç®¡å ´æ‰€CD','äº‹æ¥­CD','å•†å“CD','å®Ÿåœ¨åº«æ•°é‡'] ã‚’å«ã‚€ã“ã¨ã‚’å‰æã¨ã™ã‚‹
    """
    if uploaded_file is None:
        return pd.DataFrame()

    try:
        data = uploaded_file.getvalue()
    except Exception:
        # å¿µã®ãŸã‚
        data = uploaded_file.read()

    df = pd.read_excel(io.BytesIO(data), sheet_name=0, dtype=str)

    # åˆ—åæ­£è¦åŒ–ï¼ˆTrimç›¸å½“ + BOMé™¤å»ï¼‰
    cols = []
    for c in df.columns:
        s = str(c)
        s = s.replace("\ufeff", "")
        s = s.replace("ã€€", " ").strip()
        cols.append(s)
    df.columns = cols

    return df



def parse_dt_str(s: str) -> Optional[datetime]:
    t = s.strip()
    if not DATE_TIME_RE.match(t):
        return None
    try:
        return datetime.strptime(t, "%Y/%m/%d %H:%M:%S")
    except Exception:
        return None

def build_error_csv_bytes(details: List[ErrorDetail]) -> bytes:
    buf = StringIO()
    w = csv.writer(buf, lineterminator="\n")
    w.writerow(["è¡Œç•ªå·(ç‰©ç†è¡Œ)", "åº—èˆ—å", "ä¼ç¥¨ç•ªå·", "é‡‘é¡(38åˆ—ç›®)"])
    for d in details:
        w.writerow([d.row, d.store_name, d.slip_number, d.col_38])
    return buf.getvalue().encode("utf-8")

def build_date_issue_csv_bytes(issues: List[DateIssue]) -> bytes:
    buf = StringIO()
    w = csv.writer(buf, lineterminator="\n")
    w.writerow(["ãƒ¬ã‚³ãƒ¼ãƒ‰ç•ªå·", "é–‹å§‹ç‰©ç†è¡Œ(å‚è€ƒ)", "é‡è¦åº¦", "ç¨®åˆ¥", "9åˆ—ç›®", "17åˆ—ç›®", "è£œè¶³"])
    for it in issues:
        w.writerow([it.record_no, it.start_physical_line, it.severity, it.issue_type, it.col9, it.col17, it.note])
    return buf.getvalue().encode("utf-8")

def check_and_analyze(csv_text: str) -> Tuple[bool, List[ErrorDetail], int, int, DateSummary]:
    """
    å£²ä¸ŠCSVã®äº‹å‰æ¤œæŸ»ã€‚
    - NGæ¡ä»¶ï¼ˆ25åˆ—=Z00014 ã‹ã¤ 38åˆ—ãŒ 3000/5000 ä»¥å¤–ï¼‰ã‚’æ¤œå‡º
    - 9åˆ—ã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
    â€» æœ€å°ä¿®æ­£ï¼šå…¨è¡Œã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãŸã‚ã€returnã¯ãƒ«ãƒ¼ãƒ—å¤–ã«ç§»å‹•
    """
    error_details: List[ErrorDetail] = []
    total_data_records = 0
    total_physical_lines = 0
    total_checked_cells = 0
    count_col9_ok = 0
    count_error = 0
    issues: List[DateIssue] = []

    reader = csv_reader_from_text(csv_text)
    prev_end_line = 0

    for record_no, row in enumerate(reader, start=1):
        start_physical_line = prev_end_line + 1
        end_physical_line = reader.line_num
        prev_end_line = end_physical_line
        total_physical_lines = end_physical_line

        # skip header
        if record_no == 1:
            continue
        total_data_records += 1

        # NGãƒã‚§ãƒƒã‚¯ 25/38
        if len(row) >= (TARGET_COL_38 + 1):
            col_3 = row[2].strip() if len(row) > 2 else ""
            col_11 = row[10].strip() if len(row) > 10 else ""
            col_25 = row[TARGET_COL_25].strip() if len(row) > TARGET_COL_25 else ""
            col_38 = row[TARGET_COL_38].strip() if len(row) > TARGET_COL_38 else ""
            if col_25 == "Z00014" and col_38 not in {"3000", "5000"}:
                error_details.append(ErrorDetail(row=start_physical_line, store_name=col_3, slip_number=col_11, col_38=col_38))

        # date checks
        col9 = row[DATE_COL_9].strip() if len(row) > DATE_COL_9 else ""
        dt9 = parse_dt_str(col9)
        if dt9 is None:
            count_error += 1
            issues.append(DateIssue(
                record_no=total_data_records,
                start_physical_line=start_physical_line,
                severity="ERROR",
                issue_type="COL9_MISSING_OR_INVALID",
                col9=col9,
                col17="",
                note="9åˆ—ç›®ã« yyyy/mm/dd hh:mm:ss ãŒå¿…è¦ã§ã™ã€‚"
            ))
        else:
            count_col9_ok += 1
        total_checked_cells += 1

    # ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã«ã‚µãƒãƒªãƒ¼ä½œæˆã—ã¦è¿”ã™ï¼ˆâ†æœ€å°ä¿®æ­£ï¼‰
    date_summary = DateSummary(
        total_checked_cells=total_checked_cells,
        count_col9_ok=count_col9_ok,
        count_error=count_error,
        issues=issues
    )
    return (len(error_details) > 0), error_details, total_data_records, total_physical_lines, date_summary

### --- å–ã‚Šè¾¼ã¿ã“ã“ã¾ã§ ---

def process_shiire_data(df: pd.DataFrame, masters: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    ä»•å…¥ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨å‡¦ç†ï¼ˆPower Queryæº–æ‹ ï¼‰
    """
    if df is None or df.empty:
        return pd.DataFrame()
    # å‹å¤‰æ›
    df['å—æ‰•å‰åœ¨åº«æ•°'] = pd.to_numeric(df['å—æ‰•å‰åœ¨åº«æ•°'], errors='coerce').fillna(0).astype(int)
    df['æ•°é‡'] = pd.to_numeric(df['æ•°é‡'], errors='coerce').fillna(0).astype(int)
    df['å—æ‰•å¾Œåœ¨åº«æ•°'] = pd.to_numeric(df['å—æ‰•å¾Œåœ¨åº«æ•°'], errors='coerce').fillna(0).astype(int)
    # ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆInner Joinï¼‰
    if '857001' in masters:
        df = df.merge(masters['857001'], on='å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='inner')
    # åº—èˆ—å€‰åº«åŒºåˆ†ã§ãƒ•ã‚£ãƒ«ã‚¿
    df = df[df['åº—èˆ—å€‰åº«åŒºåˆ†'] == '1']
    # ãƒã‚¹ã‚¿857002ã¨ãƒãƒ¼ã‚¸ï¼ˆLeft Outer Joinï¼‰
    if '857002' in masters:
        # å‹ã‚’çµ±ä¸€ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰
        df['äº‹æ¥­CD'] = df['äº‹æ¥­CD'].astype(str)
        master_857002 = masters['857002'].copy()
        master_857002['äº‹æ¥­CD'] = master_857002['äº‹æ¥­CD'].astype(str)
        df = df.merge(master_857002, on=['äº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'], how='left', suffixes=('', '_master'))
        # TMSå•†å“CDãŒnullãªã‚‰å•†å“ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
        df['TMSå•†å“CD'] = df['TMSå•†å“CD'].fillna(df['å•†å“ã‚³ãƒ¼ãƒ‰'])
    else:
        df['TMSå•†å“CD'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    # æ•°é‡ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    df['æ•°é‡bk'] = df['æ•°é‡']
    # å—æ‰•ç¨®åˆ¥ã§ãƒ•ã‚£ãƒ«ã‚¿
    valid_types = ['å€‰åº«ã¸è¿”å“', 'å…¥è·', 'å…¥è·(ã‚·ã‚¹ãƒ†ãƒ è‡ªå‹•)', 'è¿”å“ã‚­ãƒ£ãƒ³ã‚»ãƒ«', 'è¿”å“ä¸å‚™']
    df = df[df['å—æ‰•ç¨®åˆ¥'].isin(valid_types)]
    # é™¤å¤–å•†å“ã‚³ãƒ¼ãƒ‰
    exclude_codes = ['ZUA292', 'ZUA34Q', 'ZUA34R', 'ZUA34S', 'ZUA34T', 'ZUA34U', 'ZUA34V', 'ZUA34W']
    for code in exclude_codes:
        df = df[~df['å•†å“ã‚³ãƒ¼ãƒ‰'].str.contains(code, na=False)]
    return df

def process_shiire_individual(df: pd.DataFrame) -> pd.DataFrame:
    """ä»•å…¥ãƒ‡ãƒ¼ã‚¿ï¼ˆå€‹ä½“æƒ…å ±ï¼‰"""
    # â† æœ€å°ä¿®æ­£ï¼šOR ã‚’è¿½åŠ ï¼ˆã„ãšã‚Œã‹ãŒéç©ºãªã‚‰å¯¾è±¡ï¼‰
    individual = df[
        (df['IMEI'].notna() & (df['IMEI'] != '')) |
        (df['ICCID'].notna() & (df['ICCID'] != '')) |
        (df['ãã®ä»–ã‚·ãƒªã‚¢ãƒ«'].notna() & (df['ãã®ä»–ã‚·ãƒªã‚¢ãƒ«'] != ''))
    ].copy()
    # ã‚«ãƒ†ã‚´ãƒªä¸­ãŒï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰ã§ãªã„
    individual = individual[individual['ã‚«ãƒ†ã‚´ãƒªä¸­'] != 'ï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰']
    # æ•°é‡è¨ˆç®—: å€‰åº«ã¸è¿”å“ãªã‚‰-1ã€ãã‚Œä»¥å¤–ã¯1
    individual['æ•°é‡'] = individual['å—æ‰•ç¨®åˆ¥'].apply(lambda x: -1 if x == 'å€‰åº«ã¸è¿”å“' else 1)
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = individual.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    return result

def process_shiire_accessory(df: pd.DataFrame) -> pd.DataFrame:
    """ä»•å…¥ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¢ã‚¯ã‚»ã‚µãƒªï¼‰"""
    # IMEIã€ICCIDã€ãã®ä»–ã‚·ãƒªã‚¢ãƒ«å…¨ã¦ãŒç©º
    accessory = df[
        (df['IMEI'].isna() | (df['IMEI'] == '')) &
        (df['ICCID'].isna() | (df['ICCID'] == '')) &
        (df['ãã®ä»–ã‚·ãƒªã‚¢ãƒ«'].isna() | (df['ãã®ä»–ã‚·ãƒªã‚¢ãƒ«'] == ''))
    ].copy()
    # æ•°é‡bkã‚’ä½¿ç”¨
    accessory['æ•°é‡'] = accessory['æ•°é‡bk']
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = accessory.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    return result

def process_ido_data(df: pd.DataFrame, masters: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    ç§»å‹•ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨å‡¦ç†ï¼ˆPower Queryæº–æ‹ ï¼‰
    """
    if df is None or df.empty:
        return pd.DataFrame()
    # å‹å¤‰æ›
    df['å…¥åº«äºˆå®šæ•°'] = pd.to_numeric(df['å…¥åº«äºˆå®šæ•°'], errors='coerce').fillna(0).astype(int)
    df['æœªå…¥åº«æ•°'] = pd.to_numeric(df['æœªå…¥åº«æ•°'], errors='coerce').fillna(0).astype(int)
    # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤ï¼ˆç©ºã®åˆ—ï¼‰
    df = df.loc[:, ~df.columns.str.startswith('_')]
    df = df.loc[:, df.columns != '']
    # ç§»å‹•å…ƒå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰ã§ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆInner Joinï¼‰
    if '857001' in masters:
        moto_master = masters['857001'].copy()
        moto_master = moto_master.rename(columns={
            'å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰': 'ç§»å‹•å…ƒå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰',
            'åº—èˆ—å€‰åº«åŒºåˆ†': 'åº—èˆ—å€‰åº«åŒºåˆ†',
            'äº‹æ¥­CD': 'ç§»å‹•å…ƒäº‹æ¥­CD',
            'ä¿ç®¡å ´æ‰€CD': 'ç§»å‹•å…ƒä¿ç®¡å ´æ‰€CD'
        })
        df = df.merge(moto_master, on='ç§»å‹•å…ƒå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='inner')
    # ç§»å‹•å…ˆå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰ã§ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆInner Joinï¼‰
    if '857001' in masters:
        saki_master = masters['857001'][['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD']].copy()
        saki_master = saki_master.rename(columns={
            'å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰': 'ç§»å‹•å…ˆå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰',
            'äº‹æ¥­CD': 'ç§»å‹•å…ˆäº‹æ¥­CD',
            'ä¿ç®¡å ´æ‰€CD': 'ç§»å‹•å…ˆä¿ç®¡å ´æ‰€CD'
        })
        df = df.merge(saki_master, on='ç§»å‹•å…ˆå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='inner')
    # ãƒã‚¹ã‚¿857002ã¨ãƒãƒ¼ã‚¸
    if '857002' in masters:
        # å‹ã‚’çµ±ä¸€ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰
        df['ç§»å‹•å…ƒäº‹æ¥­CD'] = df['ç§»å‹•å…ƒäº‹æ¥­CD'].astype(str)
        master_857002 = masters['857002'].copy()
        master_857002['äº‹æ¥­CD'] = master_857002['äº‹æ¥­CD'].astype(str)
        df = df.merge(
            master_857002,
            left_on=['ç§»å‹•å…ƒäº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'],
            right_on=['äº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'],
            how='left',
            suffixes=('', '_master')
        )
        df['TMSå•†å“CD'] = df['TMSå•†å“CD'].fillna(df['å•†å“ã‚³ãƒ¼ãƒ‰'])
        if 'äº‹æ¥­CD_master' in df.columns:
            df = df.drop(columns=['äº‹æ¥­CD_master'])
    else:
        df['TMSå•†å“CD'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    return df

def process_ido_shukko(df: pd.DataFrame) -> pd.DataFrame:
    """ç§»å‹•ãƒ‡ãƒ¼ã‚¿ï¼ˆå‡ºåº«ï¼‰"""
    # ã‚«ãƒ†ã‚´ãƒªä¸­ãŒï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰ã§ãªã„
    shukko = df[~df['ã‚«ãƒ†ã‚´ãƒªä¸­'].str.contains('ï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰', na=False)].copy()
    # æ•°é‡è¨ˆç®—: å…¥åº«äºˆå®šæ•° * -1
    shukko['æ•°é‡'] = shukko['å…¥åº«äºˆå®šæ•°'] * -1
    shukko['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'] = shukko['ç§»å‹•å…ƒå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰']
    shukko['å–æ¬¡åº—å'] = shukko['ç§»å‹•å…ƒå–æ¬¡åº—å']
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = shukko.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'ç§»å‹•å…ƒäº‹æ¥­CD', 'ç§»å‹•å…ƒä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={
        'ç§»å‹•å…ƒäº‹æ¥­CD': 'äº‹æ¥­CD',
        'ç§»å‹•å…ƒä¿ç®¡å ´æ‰€CD': 'ä¿ç®¡å ´æ‰€CD',
        'æ•°é‡': 'å¤‰å‹•æ•°'
    })
    # ä¿ç®¡å ´æ‰€CDãŒç©ºã§ãªã„
    result = result[result['ä¿ç®¡å ´æ‰€CD'] != '']
    return result

def process_ido_nyuko(df: pd.DataFrame) -> pd.DataFrame:
    """ç§»å‹•ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¥åº«ï¼‰"""
    # ã‚«ãƒ†ã‚´ãƒªä¸­ãŒï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰ã§ãªã„
    nyuko = df[~df['ã‚«ãƒ†ã‚´ãƒªä¸­'].str.contains('ï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰', na=False)].copy()
    # æ•°é‡è¨ˆç®—: å…¥åº«äºˆå®šæ•° * 1
    nyuko['æ•°é‡'] = nyuko['å…¥åº«äºˆå®šæ•°'] * 1
    nyuko['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'] = nyuko['ç§»å‹•å…ˆå–æ¬¡åº—ã‚³ãƒ¼ãƒ‰']
    nyuko['å–æ¬¡åº—å'] = nyuko['ç§»å‹•å…ˆå–æ¬¡åº—å']
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = nyuko.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'ç§»å‹•å…ˆäº‹æ¥­CD', 'ç§»å‹•å…ˆä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={
        'ç§»å‹•å…ˆäº‹æ¥­CD': 'äº‹æ¥­CD',
        'ç§»å‹•å…ˆä¿ç®¡å ´æ‰€CD': 'ä¿ç®¡å ´æ‰€CD',
        'æ•°é‡': 'å¤‰å‹•æ•°'
    })
    # ä¿ç®¡å ´æ‰€CDãŒç©ºã§ãªã„
    result = result[result['ä¿ç®¡å ´æ‰€CD'] != '']
    return result

def process_uri_data(df: pd.DataFrame, masters: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨å‡¦ç†ï¼ˆPower Queryæº–æ‹ ï¼‰
    """
    if df is None or df.empty:
        return pd.DataFrame()
    # ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆLeft Outer Joinï¼‰
    if '857001' in masters:
        df = df.merge(masters['857001'], on='å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='left')
    # å•†å“æ§‹æˆãƒã‚¹ã‚¿ã¨ã®ãƒãƒ¼ã‚¸ã¯çœç•¥ï¼ˆãƒã‚¹ã‚¿ãŒãªã„å ´åˆï¼‰
    # å•†å“ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ç”¨
    df['å•†å“ã‚³ãƒ¼ãƒ‰bk'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    # ãƒã‚¹ã‚¿857002ã¨ãƒãƒ¼ã‚¸
    if '857002' in masters:
        # å‹ã‚’çµ±ä¸€ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰
        df['äº‹æ¥­CD'] = df['äº‹æ¥­CD'].astype(str)
        master_857002 = masters['857002'].copy()
        master_857002['äº‹æ¥­CD'] = master_857002['äº‹æ¥­CD'].astype(str)
        df = df.merge(master_857002, on=['äº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'], how='left', suffixes=('', '_master'))
        df['TMSå•†å“CD'] = df['TMSå•†å“CD'].fillna(df['å•†å“ã‚³ãƒ¼ãƒ‰'])
    else:
        df['TMSå•†å“CD'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    # æ•°é‡è¨ˆç®—: åç´ç¨®åˆ¥ãŒã€Œè²©å£²ã€ãªã‚‰-1ã€ãã‚Œä»¥å¤–ã¯1
    df['æ•°é‡'] = df['åç´ç¨®åˆ¥'].apply(lambda x: -1 if x == 'è²©å£²' else 1)
    return df

def process_uri_individual(df: pd.DataFrame) -> pd.DataFrame:
    """å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆå€‹ä½“æƒ…å ±ï¼‰"""
    # å¿…è¦ãªåˆ—ãŒç„¡ã„å ´åˆã¯ç©ºæ–‡å­—åˆ—åˆ—ã‚’ä½œæˆï¼ˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ã¯åˆ—åãŒå­˜åœ¨ã—ãªã„ã“ã¨ãŒã‚ã‚‹ï¼‰
    needed = ['ãƒ¡ãƒ¼ã‚«ãƒ¼', 'æ¥­å‹™ç¨®åˆ¥', 'åº—èˆ—å', 'å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD']
    for col in needed:
        if col not in df.columns:
            df[col] = ''
    # ãƒ¡ãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹ ã‹ã¤ Apple Inc.-SBS ã¨ ï½±ï½¯ï¾Œï¾Ÿï¾™-SBS ä»¥å¤–
    individual = df[
        (df['ãƒ¡ãƒ¼ã‚«ãƒ¼'].notna()) &
        (df['ãƒ¡ãƒ¼ã‚«ãƒ¼'] != '') &
        (df['ãƒ¡ãƒ¼ã‚«ãƒ¼'] != 'Apple Inc.-SBS') &
        (df['ãƒ¡ãƒ¼ã‚«ãƒ¼'] != 'ï½±ï½¯ï¾Œï¾Ÿï¾™-SBS')
    ].copy()
    # æ¥­å‹™ç¨®åˆ¥ã«ï¼µï¼³ï¼©ï¼­ã‚’å«ã¾ãªã„
    individual = individual[~individual['æ¥­å‹™ç¨®åˆ¥'].str.contains('ï¼µï¼³ï¼©ï¼­', na=False)]
    individual['å–æ¬¡åº—å'] = individual['åº—èˆ—å']
    # æ•°é‡åˆ—ãŒç„¡ã‘ã‚Œã°ä½œæˆï¼ˆé€šå¸¸ã¯ process_uri_data ã§ä½œæˆã•ã‚Œã‚‹ï¼‰
    if 'æ•°é‡' not in individual.columns:
        if 'åç´ç¨®åˆ¥' in individual.columns:
            individual['æ•°é‡'] = individual['åç´ç¨®åˆ¥'].apply(lambda x: -1 if x == 'è²©å£²' else 1)
        else:
            individual['æ•°é‡'] = 0
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = individual.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    return result

def process_uri_sb_accessory(df: pd.DataFrame) -> pd.DataFrame:
    """å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆSBã‚¢ã‚¯ã‚»ã‚µãƒªï¼‰"""
    # å¿…è¦ãªåˆ—ãŒç„¡ã„å ´åˆã¯ç©ºæ–‡å­—åˆ—åˆ—ã‚’ä½œæˆ
    if 'å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰' not in df.columns:
        df['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'] = ''
    if 'ãƒ¡ãƒ¼ã‚«ãƒ¼' not in df.columns:
        df['ãƒ¡ãƒ¼ã‚«ãƒ¼'] = ''
    # å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰ãŒTGã§å§‹ã¾ã‚‹
    sb_acc = df[df['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰'].str.startswith('TG', na=False)].copy()
    # ãƒ¡ãƒ¼ã‚«ãƒ¼ãŒ Apple Inc.-SBS ã¾ãŸã¯ ï½±ï½¯ï¾Œï¾Ÿï¾™-SBS â† æœ€å°ä¿®æ­£ï¼šOR ã‚’è¿½åŠ 
    sb_acc = sb_acc[
        (sb_acc['ãƒ¡ãƒ¼ã‚«ãƒ¼'] == 'Apple Inc.-SBS') |
        (sb_acc['ãƒ¡ãƒ¼ã‚«ãƒ¼'] == 'ï½±ï½¯ï¾Œï¾Ÿï¾™-SBS')
    ]
    sb_acc['å–æ¬¡åº—å'] = sb_acc['åº—èˆ—å'] if 'åº—èˆ—å' in sb_acc.columns else ''
    # æ•°é‡åˆ—ãŒç„¡ã‘ã‚Œã°ä½œæˆ
    if 'æ•°é‡' not in sb_acc.columns:
        if 'åç´ç¨®åˆ¥' in sb_acc.columns:
            sb_acc['æ•°é‡'] = sb_acc['åç´ç¨®åˆ¥'].apply(lambda x: -1 if x == 'è²©å£²' else 1)
        else:
            sb_acc['æ•°é‡'] = 0
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = sb_acc.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    return result

def process_uri_service(df: pd.DataFrame) -> pd.DataFrame:
    """å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µãƒ¼ãƒ“ã‚¹ï¼‰"""
    # å¿…è¦ãªåˆ—ãŒç„¡ã„å ´åˆã¯ç©ºæ–‡å­—åˆ—åˆ—ã‚’ä½œæˆ
    if 'å•†å“åˆ†é¡' not in df.columns:
        df['å•†å“åˆ†é¡'] = ''
    if 'åº—èˆ—å' not in df.columns:
        df['åº—èˆ—å'] = ''
    # å•†å“åˆ†é¡ãŒã€Œã‚µãƒ¼ãƒ“ã‚¹ã€
    service = df[df['å•†å“åˆ†é¡'] == 'ã‚µãƒ¼ãƒ“ã‚¹'].copy()
    service['å–æ¬¡åº—å'] = service['åº—èˆ—å']
    # æ•°é‡åˆ—ãŒç„¡ã‘ã‚Œã°ä½œæˆ
    if 'æ•°é‡' not in service.columns:
        if 'åç´ç¨®åˆ¥' in service.columns:
            service['æ•°é‡'] = service['åç´ç¨®åˆ¥'].apply(lambda x: -1 if x == 'è²©å£²' else 1)
        else:
            service['æ•°é‡'] = 0
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = service.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    return result

def process_tana_data(df: pd.DataFrame, masters: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    æ£šå¸ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨å‡¦ç†ï¼ˆPower Queryæº–æ‹ ï¼‰
    """
    if df is None or df.empty:
        return pd.DataFrame()
    # ãƒã‚¹ã‚¿857001ã¨ãƒãƒ¼ã‚¸ï¼ˆInner Joinï¼‰
    if '857001' in masters:
        df = df.merge(masters['857001'], on='å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', how='inner')
    # å‹å¤‰æ›
    df['å—æ‰•å‰åœ¨åº«æ•°'] = pd.to_numeric(df['å—æ‰•å‰åœ¨åº«æ•°'], errors='coerce').fillna(0).astype(int)
    df['æ•°é‡'] = pd.to_numeric(df['æ•°é‡'], errors='coerce').fillna(0).astype(int)
    df['å—æ‰•å¾Œåœ¨åº«æ•°'] = pd.to_numeric(df['å—æ‰•å¾Œåœ¨åº«æ•°'], errors='coerce').fillna(0).astype(int)
    # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
    df = df.loc[:, ~df.columns.str.startswith('_')]
    df = df.loc[:, df.columns != '']
    # åº—èˆ—å€‰åº«åŒºåˆ†ã§ãƒ•ã‚£ãƒ«ã‚¿
    df = df[df['åº—èˆ—å€‰åº«åŒºåˆ†'] == '1']
    # ãƒã‚¹ã‚¿857002ã¨ãƒãƒ¼ã‚¸
    if '857002' in masters:
        # å‹ã‚’çµ±ä¸€ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰
        df['äº‹æ¥­CD'] = df['äº‹æ¥­CD'].astype(str)
        master_857002 = masters['857002'].copy()
        master_857002['äº‹æ¥­CD'] = master_857002['äº‹æ¥­CD'].astype(str)
        df = df.merge(master_857002, on=['äº‹æ¥­CD', 'å•†å“ã‚³ãƒ¼ãƒ‰'], how='left', suffixes=('', '_master'))
        df['TMSå•†å“CD'] = df['TMSå•†å“CD'].fillna(df['å•†å“ã‚³ãƒ¼ãƒ‰'])
    else:
        df['TMSå•†å“CD'] = df['å•†å“ã‚³ãƒ¼ãƒ‰']
    # é™¤å¤–å•†å“ã‚³ãƒ¼ãƒ‰
    exclude_codes = ['ZUA292', 'ZUA34Q', 'ZUA34R', 'ZUA34S', 'ZUA34T', 'ZUA34U', 'ZUA34V', 'ZUA34W']
    for code in exclude_codes:
        df = df[~df['å•†å“ã‚³ãƒ¼ãƒ‰'].str.contains(code, na=False)]
    return df

def process_tana_grouped(df: pd.DataFrame) -> pd.DataFrame:
    """æ£šå¸ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰"""
    # ã‚«ãƒ†ã‚´ãƒªä¸­ãŒï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰ã§ãªã„
    grouped = df[~df['ã‚«ãƒ†ã‚´ãƒªä¸­'].str.contains('ï¼µï¼³ï¼©ï¼­ã‚«ãƒ¼ãƒ‰', na=False)].copy()
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    result = grouped.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'TMSå•†å“CD'],
        dropna=False
    )['æ•°é‡'].sum().reset_index()
    result = result.rename(columns={'æ•°é‡': 'å¤‰å‹•æ•°'})
    return result

def combine_all_data(shiire_ind, shiire_acc, ido_shukko, ido_nyuko,
                     uri_ind, uri_sb, uri_service, tana_grouped) -> pd.DataFrame:
    """
    å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦TMSå•†å“CDã§é›†è¨ˆï¼ˆGINIEPOSå¤‰å‹•æ•°ï¼‰
    """
    all_dfs = []
    for df in [shiire_ind, shiire_acc, ido_shukko, ido_nyuko, uri_ind, uri_sb, uri_service, tana_grouped]:
        if df is not None and not df.empty:
            all_dfs.append(df)
    if not all_dfs:
        return pd.DataFrame()
    # å…¨ã¦çµåˆ
    combined = pd.concat(all_dfs, ignore_index=True)
    # TMSå•†å“CDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦åˆè¨ˆ
    result = combined.groupby(
        ['å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰', 'å–æ¬¡åº—å', 'äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'TMSå•†å“CD'],
        dropna=False
    )['å¤‰å‹•æ•°'].sum().reset_index()
    # ã‚½ãƒ¼ãƒˆ
    result = result.sort_values('å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰').reset_index(drop=True)
    return result

def compare_with_current_inventory(giniepos_df: pd.DataFrame, current_df: pd.DataFrame) -> pd.DataFrame:
    """
    GINIEPOSå¤‰å‹•æ•°ã¨ç¾åœ¨åº«ç…§ä¼šã‚’æ¯”è¼ƒï¼ˆåˆ¤å®šçµæœï¼‰
    - PowerQueryæº–æ‹ ã§ã€ã‚­ãƒ¼åˆ—ã¯TRIMç›¸å½“ï¼ˆå‰å¾Œç©ºç™½/å…¨è§’ç©ºç™½é™¤å»ï¼‰ã—ã¦çªåˆã‚ºãƒ¬ã‚’é˜²ã
    - ç¾åœ¨åº«ã®å®Ÿåœ¨åº«æ•°é‡ï¼ˆæ–‡å­—ã®å ´åˆã‚ã‚Šï¼‰ã‚’æ•°å€¤åŒ–ã—ã¦åˆ¤å®šã«ä½¿ç”¨ã™ã‚‹
    """
    if giniepos_df is None or giniepos_df.empty or current_df is None or current_df.empty:
        return pd.DataFrame()

    # ç¾åœ¨åº«ç…§ä¼šã‹ã‚‰å¿…è¦ãªåˆ—ã®ã¿å–å¾—
    required_cols = ['ä¿ç®¡å ´æ‰€CD', 'äº‹æ¥­CD', 'å•†å“CD', 'å®Ÿåœ¨åº«æ•°é‡']
    missing = [c for c in required_cols if c not in current_df.columns]
    if missing:
        raise KeyError(f"ç¾åœ¨åº«ç…§ä¼šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing} / å®Ÿåˆ—={list(current_df.columns)}")

    current_summary = current_df[required_cols].copy()

    # ---- ã‚­ãƒ¼åˆ—ã®æ­£è¦åŒ–ï¼ˆPowerQueryã®Trimç›¸å½“ï¼‰ ----
    for c in ['äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'å•†å“CD']:
        current_summary[c] = normalize_key_series(current_summary[c])
    for c in ['äº‹æ¥­CD', 'ä¿ç®¡å ´æ‰€CD', 'TMSå•†å“CD']:
        giniepos_df[c] = normalize_key_series(giniepos_df[c])

    # ---- åœ¨åº«æ•°é‡ã®æ•°å€¤åŒ–ï¼ˆæ–‡å­—â†’æ•°å€¤ï¼‰ ----
    # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š/ç©ºç™½æ··å…¥ã‚’è¨±å®¹
    current_summary['CLå®Ÿåœ¨åº«æ•°'] = (
        current_summary['å®Ÿåœ¨åº«æ•°é‡']
        .astype(str)
        .str.replace(',', '', regex=False)
        .str.replace('ã€€', ' ', regex=False)
        .str.strip()
    )
    current_summary['CLå®Ÿåœ¨åº«æ•°'] = pd.to_numeric(current_summary['CLå®Ÿåœ¨åº«æ•°'], errors='coerce').fillna(0)

    # ãƒãƒ¼ã‚¸ï¼ˆLeft Outer Joinï¼‰
    result = giniepos_df.merge(
        current_summary[['ä¿ç®¡å ´æ‰€CD', 'äº‹æ¥­CD', 'å•†å“CD', 'CLå®Ÿåœ¨åº«æ•°']],
        left_on=['ä¿ç®¡å ´æ‰€CD', 'äº‹æ¥­CD', 'TMSå•†å“CD'],
        right_on=['ä¿ç®¡å ´æ‰€CD', 'äº‹æ¥­CD', 'å•†å“CD'],
        how='left'
    )

    # ãƒãƒ¼ã‚¸ã§ããªã‹ã£ãŸå ´åˆã¯ç¾åœ¨åº«0æ‰±ã„ï¼ˆPowerQueryã§ã‚‚nullâ†’0ç›¸å½“ï¼‰
    result['CLå®Ÿåœ¨åº«æ•°'] = result['CLå®Ÿåœ¨åº«æ•°'].fillna(0)

    # åˆ¤å®š = å¤‰å‹•æ•° + å®Ÿåœ¨åº«
    result['åˆ¤å®š'] = result['å¤‰å‹•æ•°'] + result['CLå®Ÿåœ¨åº«æ•°']

    # é™¤å¤–ï¼ˆPowerQueryæº–æ‹ ï¼‰
    result = result[~result['TMSå•†å“CD'].str.contains('BB-RQ8POU1740', na=False)]
    result = result[~result['TMSå•†å“CD'].str.contains('ZUA292', na=False)]

    # åœ¨åº«ä¸è¶³ã®ã¿
    result = result[result['åˆ¤å®š'] < 0]

    # ã•ã‚‰ã«é™¤å¤–
    result = result[~result['TMSå•†å“CD'].str.contains('Z00014', na=False)]
    result = result[~result['TMSå•†å“CD'].str.contains('POS-', na=False)]
    return result

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³
st.markdown("""
<style>
/* ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¦‹å‡ºã—ã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã« */
.precheck-upload-title{
  font-size: 1.25rem;
  font-weight: 700;
  margin: 0.2rem 0 0.6rem 0;
  line-height: 1.2;
}
.precheck-step{
  display:inline-block;
  font-size:0.95rem;
  font-weight:700;
  padding:2px 10px;
  border-radius:999px;
  background:#eef2ff;
  margin-right:10px;
}
</style>
<div class="precheck-upload-title"><span class="precheck-step">1</span>ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</div>
""", unsafe_allow_html=True)

# file_uploader ã‚’åºƒãã™ã‚‹ï¼ˆè½ã¨ã—ã‚„ã™ãã™ã‚‹ï¼‰
st.markdown(
    """
<style>
/* Make the file drop zone taller and easier to use (robust across Streamlit versions) */
div[data-testid="stFileUploader"]{
    min-height: 240px;
}
div[data-testid="stFileUploader"] > div{
    min-height: 240px;
}
div[data-testid="stFileUploader"] section{
    padding-top: 24px;
    padding-bottom: 24px;
}
div[data-testid="stFileUploader"] section > div{
    min-height: 240px;
    display: flex;
    align-items: center;
}
/* Newer Streamlit builds use a dedicated dropzone testid */
div[data-testid="stFileUploaderDropzone"]{
    min-height: 240px !important;
    padding-top: 24px;
    padding-bottom: 24px;
    display: flex;
    align-items: center;
}
</style>
""",
    unsafe_allow_html=True,
)

uploaded_files = st.file_uploader(
    "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã¾ãŸã¯ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—",
    type=['csv', 'xlsx', 'xls'],
    accept_multiple_files=True,
    help="åœ¨åº«å¤‰å‹•ãƒ‡ãƒ¼ã‚¿4ãƒ•ã‚¡ã‚¤ãƒ« + ç¾åœ¨åº«ç…§ä¼š + ãƒã‚¹ã‚¿3ãƒ•ã‚¡ã‚¤ãƒ« = è¨ˆ8ãƒ•ã‚¡ã‚¤ãƒ«",
    key=f"uploader_{st.session_state.uploader_version}"
)


# ã‚¯ãƒªã‚¢ï¼ˆçµæœã ã‘æ¶ˆã—ã¦åˆæœŸçŠ¶æ…‹ã«æˆ»ã™ï¼‰
if st.button("ğŸ”„ ã‚¯ãƒªã‚¢", key="clear_btn", help="çµæœã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢ã—ã¦åˆæœŸè¡¨ç¤ºã«æˆ»ã—ã¾ã™"):
    st.session_state.processed_data = None
    st.session_state.last_full_sig = None
    st.session_state.uploader_version += 1
    safe_rerun()

# ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ¯ã‚Šåˆ†ã‘
shiire_file = None
ido_file = None
uri_file = None
tana_file = None
current_file = None
master_857001_file = None
master_857002_file = None
master_857003_file = None

if uploaded_files:
    for file in uploaded_files:
        filename = file.name
        if 'SHI' in filename.upper():
            shiire_file = file

        elif 'IDO' in filename.upper():
            ido_file = file

        elif 'URI' in filename.upper():
            uri_file = file

        elif 'TNA' in filename.upper():
            tana_file = file

        elif 'ç¾åœ¨åº«' in filename or 'ZAIKO' in filename.upper():
            current_file = file

        elif '857001' in filename:
            master_857001_file = file

        elif '857002' in filename:
            master_857002_file = file

        elif '857003' in filename:
            master_857003_file = file

        else:
            st.warning(f"âš ï¸ ä¸æ˜ãªãƒ•ã‚¡ã‚¤ãƒ«: {filename}")

    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãƒã‚§ãƒƒã‚¯


# å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
total_files = sum([
    shiire_file is not None,
    ido_file is not None,
    uri_file is not None,
    tana_file is not None,
    current_file is not None,
    master_857001_file is not None,
    master_857002_file is not None,
    master_857003_file is not None
])

if total_files < 8:
    st.warning(f"âš ï¸ {total_files}/8ãƒ•ã‚¡ã‚¤ãƒ«ãŒèªè­˜ã•ã‚Œã¾ã—ãŸã€‚å…¨8ãƒ•ã‚¡ã‚¤ãƒ«å¿…è¦ã§ã™ã€‚")
else:
    # 8ãƒ•ã‚¡ã‚¤ãƒ«æƒã£ãŸã‚‰è‡ªå‹•ã§å‡¦ç†ï¼ˆé€”ä¸­ãƒ­ã‚°ã¯å‡ºã•ãªã„ï¼‰
    def run_full_check(shiire_file, ido_file, uri_file, tana_file, current_file, master_857001_file, master_857002_file, master_857003_file):
        result = {
            "charge": {"label": "ãƒãƒ£ãƒ¼ã‚¸é‡‘é¡ãƒã‚§ãƒƒã‚¯", "status": "æœªå®Ÿè¡Œ", "table": None, "message": ""},
            "date":   {"label": "å£²ä¸Šæ—¥ä»˜ãƒã‚§ãƒƒã‚¯",     "status": "æœªå®Ÿè¡Œ", "table": None, "message": ""},
            "inv":    {"label": "åœ¨åº«ä¸è¶³ãƒã‚§ãƒƒã‚¯",     "status": "æœªå®Ÿè¡Œ", "table": None, "message": ""},
        }

        # ---- ã¾ãšå£²ä¸Šï¼ˆURIï¼‰ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒãƒ£ãƒ¼ã‚¸é‡‘é¡ & æ—¥ä»˜ï¼‰ ----
        try:
            uri_bytes = uri_file.getvalue()
            # æ–‡å­—åŒ–ã‘å›é¿: UTF-8(BOMã‚ã‚Š/ãªã—)å„ªå…ˆã€ã ã‚ãªã‚‰cp932
            try:
                uri_text = uri_bytes.decode("utf-8-sig")
            except Exception:
                uri_text = uri_bytes.decode("cp932", errors="replace")

            err_flag, err_details, _, _, date_summary = check_and_analyze(uri_text)

            # ãƒãƒ£ãƒ¼ã‚¸é‡‘é¡ãƒã‚§ãƒƒã‚¯
            if err_flag:
                import pandas as pd
                df_err = pd.DataFrame([{
                    "è¡Œç•ªå·(ç‰©ç†è¡Œ)": d.row,
                    "åº—èˆ—å": d.store_name,
                    "ä¼ç¥¨ç•ªå·": d.slip_number,
                    "é‡‘é¡(38åˆ—ç›®)": d.col_38
                } for d in err_details])
                result["charge"]["status"] = "NG"
                result["charge"]["table"] = df_err
                result["charge"]["message"] = f"NG {len(df_err)}ä»¶"
            else:
                result["charge"]["status"] = "OK"
                result["charge"]["message"] = "OK"

            # å£²ä¸Šæ—¥ä»˜ãƒã‚§ãƒƒã‚¯ï¼ˆæŒ‡æ‘˜ãŒã‚ã‚Œã°è­¦å‘Šæ‰±ã„ï¼‰
            if date_summary and getattr(date_summary, "issues", None):
                import pandas as pd
                df_issue = pd.DataFrame([{
                    "ãƒ¬ã‚³ãƒ¼ãƒ‰ç•ªå·": it.record_no,
                    "é–‹å§‹ç‰©ç†è¡Œ(å‚è€ƒ)": it.start_physical_line,
                    "é‡è¦åº¦": it.severity,
                    "ç¨®åˆ¥": it.issue_type,
                    "9åˆ—ç›®": it.col9,
                    "17åˆ—ç›®": it.col17,
                    "è£œè¶³": it.note
                } for it in date_summary.issues])
                result["date"]["status"] = "NG"
                result["date"]["table"] = df_issue
                result["date"]["message"] = f"NG {len(df_issue)}ä»¶"
            else:
                result["date"]["status"] = "OK"
                result["date"]["message"] = "OK"

            # å£²ä¸ŠãŒNGãªã‚‰ä»¥é™ã¯æœªå®Ÿè¡Œï¼ˆä¾å­˜é–¢ä¿‚ãŒã‚ã‚‹ãŸã‚ï¼‰
            if result["charge"]["status"] == "NG" or result["date"]["status"] == "NG":
                return result

        except Exception as e:
            result["charge"]["status"] = "NG"
            result["charge"]["message"] = f"å£²ä¸Šãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {e}"
            return result

        # ---- ã“ã“ã‹ã‚‰åœ¨åº«ä¸è¶³ãƒã‚§ãƒƒã‚¯ï¼ˆä¾å­˜ï¼šå£²ä¸ŠãŒæ­£å¸¸ï¼‰ ----
        try:
            masters = load_master_files(master_857001_file, master_857002_file, master_857003_file)

            # CSVèª­ã¿è¾¼ã¿ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’å°Šé‡ï¼‰
            shiire_df = load_csv_with_encoding(shiire_file, use_lf=False, encoding="cp932")
            ido_df = load_csv_with_encoding(ido_file, use_lf=False, encoding="cp932")
            tana_df = load_csv_with_encoding(tana_file, use_lf=False, encoding="cp932")

            # URIã¯ä¸Šã§è§£ææ¸ˆã¿ã ãŒã€æ—¢å­˜å‡¦ç†é–¢æ•°ã¯UploadFileå‰æãªã®ã§ã“ã“ã§DataFrameåŒ–
            uri_df = load_csv_from_text(uri_text)

            current_df = load_current_inventory_excel(current_file)

            # æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
            shiire_processed = process_shiire_data(shiire_df, masters)
            shiire_individual = process_shiire_individual(shiire_processed)
            shiire_accessory = process_shiire_accessory(shiire_processed)

            ido_processed = process_ido_data(ido_df, masters)
            ido_shukko = process_ido_shukko(ido_processed)
            ido_nyuko = process_ido_nyuko(ido_processed)

            uri_processed = process_uri_data(uri_df, masters)
            uri_individual = process_uri_individual(uri_processed)
            uri_sb = process_uri_sb_accessory(uri_processed)
            uri_service = process_uri_service(uri_processed)

            tana_processed = process_tana_data(tana_df, masters)
            tana_grouped = process_tana_grouped(tana_processed)

            # å¤‰å‹•æ•°é›†ç´„ï¼ˆæ—¢å­˜é–¢æ•°ã‚’ä½¿ç”¨ï¼‰
            merged_hendo = combine_all_data(
                shiire_individual, shiire_accessory,
                ido_shukko, ido_nyuko,
                uri_individual, uri_sb, uri_service,
                tana_grouped
            )
            inv_result = compare_with_current_inventory(merged_hendo, current_df)

            import pandas as pd
            if inv_result is None or len(inv_result) == 0:
                result["inv"]["status"] = "OK"
                result["inv"]["message"] = "OK"
                result["inv"]["table"] = pd.DataFrame()
            else:
                result["inv"]["status"] = "NG"
                result["inv"]["message"] = f"NG {len(inv_result)}ä»¶"
                # è¡¨ç¤ºç”¨ã«æœ€ä½é™ã®åˆ—ã«çµã‚‹ï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘ï¼‰
                show_cols = ["å–æ¬¡åº—ã‚³ãƒ¼ãƒ‰","å–æ¬¡åº—å","TMSå•†å“CD","å¤‰å‹•æ•°","CLå®Ÿåœ¨åº«æ•°","åˆ¤å®š"]
                avail = [c for c in show_cols if c in inv_result.columns]
                result["inv"]["table"] = inv_result[avail].copy()

        except Exception as e:
            result["inv"]["status"] = "NG"
            result["inv"]["message"] = f"åœ¨åº«ä¸è¶³ãƒã‚§ãƒƒã‚¯ï¼ˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼‰: {e}"
            result["inv"]["table"] = pd.DataFrame()

        return result

    # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ãƒƒãƒˆã§äºŒé‡å®Ÿè¡Œã—ãªã„
    sig_parts = [
        getattr(shiire_file, "name", None),
        getattr(ido_file, "name", None),
        getattr(uri_file, "name", None),
        getattr(tana_file, "name", None),
        getattr(current_file, "name", None),
        getattr(master_857001_file, "name", None),
        getattr(master_857002_file, "name", None),
        getattr(master_857003_file, "name", None),
    ]
    sig = tuple(sig_parts)
    if "last_full_sig" not in st.session_state:
        st.session_state.last_full_sig = None

    if st.session_state.last_full_sig != sig or st.session_state.processed_data is None:
        st.session_state.last_full_sig = sig
        with st.spinner("å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™..."):
            st.session_state.processed_data = run_full_check(
                shiire_file, ido_file, uri_file, tana_file, current_file,
                master_857001_file, master_857002_file, master_857003_file
            )
        st.success("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")

# ----------------------------
# ç¢ºèªçµæœï¼ˆ3ã¤ã®ãƒã‚§ãƒƒã‚¯ã‚’å·¦â†’å³ã§è¡¨ç¤ºï¼‰
# ----------------------------
if st.session_state.processed_data:
    st.markdown("---")
    st.header("2ï¸âƒ£ ç¢ºèªçµæœ")

    data = st.session_state.processed_data

    def _status_badge(status: str) -> str:
        if status == "OK":
            return "<span style='font-size:42px; font-weight:800; color:#1f7a1f;'>OK</span>"
        if status == "NG":
            return "<span style='font-size:42px; font-weight:800; color:#d11a2a;'>NG</span>"
        return "<span style='font-size:42px; font-weight:800; color:#666;'>æœªå®Ÿè¡Œ</span>"

    cols = st.columns(3)
    for i, key in enumerate(["charge", "date", "inv"]):
        with cols[i]:
            st.markdown(f"**{data[key]['label']}**", unsafe_allow_html=True)
            st.markdown(_status_badge(data[key]["status"]), unsafe_allow_html=True)
            if data[key].get("message"):
                st.caption(data[key]["message"])

    # ã‚¨ãƒ©ãƒ¼è©³ç´°ï¼ˆNGã®ã¿ã€è¡¨ç¤ºé †ã‚‚å·¦â†’å³ï¼‰
    st.markdown("---")
    any_ng = any(data[k]["status"] == "NG" for k in ["charge","date","inv"])
    if any_ng:
        st.subheader("ğŸ“Œ ã‚¨ãƒ©ãƒ¼è©³ç´°")
        for key in ["charge","date","inv"]:
            if data[key]["status"] != "NG":
                continue
            st.markdown(f"### {data[key]['label']}")
            tbl = data[key].get("table")
            if tbl is None:
                st.write(data[key].get("message",""))
                continue
            if hasattr(tbl, "empty") and tbl.empty:
                st.write("ï¼ˆè©²å½“ãªã—ï¼‰")
                continue
            st.dataframe(tbl, use_container_width=True, height=300)
    else:
        st.success("âœ… ã™ã¹ã¦OKã§ã™")
